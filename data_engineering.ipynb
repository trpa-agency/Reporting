{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TO DO\n",
    "* Add ADU, Bonus Unit, and Allocation fields to Parcel_History_Attributed\n",
    "* Clean up unnecessary fields in Parcel_History_Attributed\n",
    "* Add VHR/Bedrooms to the data\n",
    "* Add CFA research from Ken to the data\n",
    "* Check totals year over year\n",
    "* Add in way to model TAUs as Residential units where City converted Hotels>Apartments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terminology "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data engineering can consist of ***collection, cleaning, transformation, processing, and automating and monitoring tasks***\n",
    "* Collection - examples include getting data from a rest service as a\n",
    "* Cleaning - categorizing \n",
    "* Transformation - cateogorizing, standardization, \n",
    "* Processing - algorithm, pivot, groupby, merge\n",
    "* Automating - schedule task, Apache Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Planning Jargon\n",
    "* ADU - Accessory Dwelling Unit\n",
    "* Existing Development Right - refers to residential, commercial, or tourist development currently built in the Lake Tahoe Basin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import arcpy\n",
    "from arcgis.features import FeatureLayer, GeoAccessor, GeoSeriesAccessor\n",
    "from arcgis.mapping import show_styles, display_colormaps\n",
    "from arcgis.gis import GIS\n",
    "from utils import *\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "import datetime\n",
    "from time import strftime  \n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fieldJoinCalc_multikey(updateFC, updateFieldsList_key, updateFieldsList_value, sourceFC, sourceFieldsList_key, sourceFieldsList_value):\n",
    "    from time import strftime  \n",
    "    print (\"Started data transfer: \" + strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    # Use list comprehension to build a dictionary from arcpy SearchCursor  \n",
    "    total_count=0\n",
    "    valueDict = {(r[0]+str(r[1])):(r[2]) for r in arcpy.da.SearchCursor(sourceFC, (sourceFieldsList_key + sourceFieldsList_value)) if r[0] is not None and r[1] is not None}  \n",
    "    with arcpy.da.UpdateCursor(updateFC, (updateFieldsList_key + updateFieldsList_value)) as updateRows:  \n",
    "        for updateRow in updateRows:  \n",
    "            # store the Join value of the row being updated in a keyValue variable  \n",
    "            if updateRow[0] is not None and updateRow[1] is not None:\n",
    "                keyValue = updateRow[0]+str(updateRow[1])\n",
    "                # verify that the keyValue is in the Dictionary  \n",
    "                if keyValue in valueDict:\n",
    "                    total_count +=1\n",
    "                    if (total_count%1000)==0:\n",
    "                        print (f\"Updating row {total_count}\")\n",
    "                    # transfer the value stored under the keyValue from the dictionary to the updated field.  \n",
    "                    updateRow[2] = valueDict[keyValue]  \n",
    "                    updateRows.updateRow(updateRow)    \n",
    "    del valueDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data frame display options\n",
    "# pandas options\n",
    "pd.options.mode.copy_on_write = True\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows    = 999\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "   \n",
    "# set environement workspace to in memory \n",
    "arcpy.env.workspace = 'memory'\n",
    "# overwrite true\n",
    "arcpy.env.overwriteOutput = True\n",
    "# Set spatial reference to NAD 1983 UTM Zone 10N\n",
    "sr = arcpy.SpatialReference(26910)\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "# # Set the extent environment using a feature class\n",
    "# arcpy.env.extent = \"TRPA_Boundary\"\n",
    "\n",
    "# current working directory\n",
    "local_path = pathlib.Path().absolute()\n",
    "# set data path as a subfolder of the current working directory TravelDemandModel\\2022\\\n",
    "data_dir   = local_path.parents[0] / 'Reporting/data/raw_data'\n",
    "# folder to save processed data\n",
    "out_dir    = local_path.parents[0] / 'Reporting/data/processed_data'\n",
    "# workspace gdb for stuff that doesnt work in memory\n",
    "gdb        = local_path.parents[0] / 'Reporting/data/Workspace.gdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(local_path)\n",
    "print(data_dir)\n",
    "print(out_dir)\n",
    "print(gdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the GIS object\n",
    "## portal URL = \"https://maps.trpa.org/portal/home/\"\n",
    "## AGOL URL   = \"https://www.arcgis.com\"\n",
    "gis = GIS(\n",
    "    url=\"https://maps.trpa.org/portal/home/\",\n",
    "    ## enter username above ##\n",
    "    username= input(\"Enter username:\"),\n",
    "    ## enter password above ##\n",
    "    password=getpass.getpass(\"Enter password:\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a map object\n",
    "map = gis.map(\"Lake Tahoe\", zoomlevel=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sources\n",
    "* https://www.laketahoeinfo.org/WebServices/List\n",
    "* https://maps.trpa.org/server/rest/services/\n",
    "* sdeBase, sdeCollect, sdeTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature classes from sde\n",
    "sde_ParcelAtt    = os.path.join(sdeCollect, \"\\\\SDE.Parcel\\\\SDE.Parcel_History_Attributed\")\n",
    "sde_ParcelMaster = os.path.join(sdeBase,\"\\\\sde.SDE.Parcels\\\\sde.SDE.Parcel_Master\")\n",
    "\n",
    "## get parcel data\n",
    "sdfParcel     = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Existing_Development/MapServer/2\")\n",
    "sdfParcel23   = get_fs_data_spatial_query(\"https://maps.trpa.org/server/rest/services/Existing_Development/MapServer/2\", \"YEAR = 2023\") \n",
    "# get spatial data to join to\n",
    "sdfDistrict   = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Planning/MapServer/1\")\n",
    "sdfPlan       = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/MapServer/0\")\n",
    "sdfTownCenter = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/MapServer/1\")\n",
    "sdfTCbuffer   = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Planning/MapServer/4\")\n",
    "sdfCSLT       = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/MapServer/2\")\n",
    "sdfCounty     = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/MapServer/3\")\n",
    "sdfTRPA       = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/MapServer/4\")\n",
    "\n",
    "# set spatial reference\n",
    "sdfParcel.spatial.sr = sr\n",
    "sdfParcel23.spatial.sr = sr\n",
    "sdfDistrict.spatial.sr = sr\n",
    "sdfPlan.spatial.sr = sr\n",
    "sdfTownCenter.spatial.sr = sr\n",
    "sdfTCbuffer.spatial.sr = sr\n",
    "sdfCSLT.spatial.sr = sr\n",
    "sdfCounty.spatial.sr = sr\n",
    "sdfTRPA.spatial.sr = sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network path to connection files\n",
    "filePath = \"F:/GIS/PARCELUPDATE/Workspace/\"\n",
    "# database file path \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "sdeTabular = os.path.join(filePath, \"Tabular.sde\")\n",
    "# feature classes from sde\n",
    "sde_ParcelAtt        = sdeCollect + \"\\\\SDE.Parcel\\\\SDE.Parcel_History_Attributed\"\n",
    "sde_LocalPlan        = sdeBase + \"\\\\sde.SDE.Planning\\\\sde.SDE.LocalPlan\"\n",
    "sde_CSLT             = sdeBase + \"\\\\sde.SDE.Jurisdictions\\\\sde.SDE.CSLT\"\n",
    "sde_CurrentParcels   = sdeBase + \"\\\\sde.SDE.Parcels\\\\sde.SDE.Parcel_Master\"\n",
    "sde_District         = sdeBase + \"\\\\sde.SDE.Planning\\\\sde.SDE.District\"\n",
    "sde_TownCenter       = sdeBase + \"\\\\sde.SDE.Planning\\\\sde.SDE.TownCenter\"\n",
    "sde_TownCenterBuffer = sdeBase + \"\\\\sde.SDE.Planning\\\\sde.SDE.TownCenter_Buffer\"\n",
    "sde_TRPAboundary     = sdeBase + \"\\\\sde.SDE.Jurisdictions\\\\sde.SDE.TRPA_bdy\"\n",
    "sde_BonusUnitboundary= sdeBase + \"\\\\sde.SDE.Planning\\\\sde.SDE.Bonus_unit_boundary\"\n",
    "sde_UrbanArea        = sdeBase + \"\\\\sde.SDE.Jurisdictions\\\\sde.SDE.UrbanAreas\"\n",
    "sde_Zip              = sdeBase + \"\\\\sde.SDE.Jurisdictions\\\\sde.SDE.Postal_ZIP\"\n",
    "sde_TAZ              = sdeBase + \"\\\\sde.SDE.Transportation\\\\sde.SDE.Transportation_Analysis_Zone\"\n",
    "sdf_County           = sdeBase + \"\\\\sde.SDE.Jurisdictions\\\\SDE.Counties\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LT Info Data\n",
    "# Verified Development Rights\n",
    "dfDevRight  = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetParcelDevelopmentRightsForAccela/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "# Deed Restrictions as a DataFrame\n",
    "dfDeed      = pd.read_json(\"https://laketahoeinfo.org/WebServices/GetDeedRestrictedParcels/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "# IPES LTinfo as a DataFrame\n",
    "dfIPES      = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetParcelIPESScores/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "# Development Rights Transacted and Banked as a DataFrame\n",
    "dfDevRights = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetTransactedAndBankedDevelopmentRights/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "# All Parcels as a DataFrame\n",
    "dfLTParcel  = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetAllParcels/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get 2022 development units\n",
    "devhistoryURL = \"https://maps.trpa.org/server/rest/services/Existing_Development/MapServer/2\"\n",
    "# get parcel history for 2023\n",
    "# df23 = get_fs_data_query(devhistoryURL, \"Year = 2023\")\n",
    "# df22 = get_fs_data_query(devhistoryURL, \"Year = 2022\")\n",
    "# df21 = get_fs_data_query(devhistoryURL, \"Year = 2021\")\n",
    "# df20 = get_fs_data_query(devhistoryURL, \"Year = 2020\")\n",
    "# df19 = get_fs_data_query(devhistoryURL, \"Year = 2019\")\n",
    "# df18 = get_fs_data_query(devhistoryURL, \"Year = 2018\")\n",
    "df12 = get_fs_data_query(devhistoryURL, \"Year = 2012\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set spatial reference\n",
    "sdfParcel.spatial.sr = sr\n",
    "sdfDistrict.spatial.sr = sr\n",
    "sdfPlan.spatial.sr = sr\n",
    "sdfTownCenter.spatial.sr = sr\n",
    "sdfTCbuffer.spatial.sr = sr\n",
    "sdfCSLT.spatial.sr = sr\n",
    "sdfCounty.spatial.sr = sr\n",
    "sdfTRPA.spatial.sr = sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web service url\n",
    "permitTable = \"https://maps.trpa.org/server/rest/services/Permit_Records/MapServer/1\"\n",
    "# get permit data as a dataframe\n",
    "dfTRPAPermit = get_fs_data(permitTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permit Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRPA Permit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Data***\n",
    "> TRPA permit data is exported from accela nightly then stored in colleciton.sde enterprise geodatabase and published to the trpa server as the web service below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRPA Permit Data Engineering\n",
    "dfTRPAPermit.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Transformation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfTRPAPermit\n",
    "\n",
    "# final fields for all permit dataframes\n",
    "fields = ['APN', 'Address', 'Jurisdiction', 'Permit_ID', \n",
    "          'Permit_Type','Permit_Category', 'Permit_Status',  'Description',\n",
    "          'Applied_Date', 'Issued_Date', 'PreGrade_Date', 'Finaled_Date'\n",
    "          ]\n",
    "\n",
    "# # set fields\n",
    "column_mapping = {\n",
    "'Accela_ID' : 'Permit_ID',\n",
    "'Detailed_Description' : 'Description',\n",
    "'Record_Status' : 'Permit_Status',\n",
    "'Accela_CAPType_Name' : 'Permit_Type',\n",
    "'File_Date' : 'Applied_Date'\n",
    "}\n",
    "\n",
    "# rename columns based on dictionary\n",
    "df = renamecolumns(df, column_mapping, False)\n",
    "\n",
    "# add missing fields\n",
    "for field in fields:\n",
    "    # if field not in dataframe add it\n",
    "    if field not in df.columns:\n",
    "        # insert new column\n",
    "        df[field] = None\n",
    "# limit to the final fields\n",
    "df = df[fields]\n",
    "# add jurisdiction value\n",
    "df.Jurisdiction = \"TRPA\"\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Processing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out unique Record_Status values one at a time\n",
    "for description in dfTRPAPermit.Detailed_Description.unique():\n",
    "    print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out unique Record_Status values one at a time\n",
    "for permittype in dfTRPAPermit.Accela_CAPType_Name.unique():\n",
    "    print(permittype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out unique Record_Status values one at a time\n",
    "for status in dfTRPAPermit.Record_Status.unique():\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_lookup = \"resources\\Value_Lookups.csv\"\n",
    "trpa_reportingcategory_lookup = import_lookup_dictionary(value_lookup,'key','value','Jurisdiction','TRPA','FieldName','Reporting_Category')\n",
    "trpa_permittype_lookup        = import_lookup_dictionary(value_lookup,'key','value','Jurisdiction','TRPA','FieldName','Permit_Type')\n",
    "trpa_permitstatus_lookup      = import_lookup_dictionary(value_lookup,'key','value','Jurisdiction','TRPA','FieldName','Permit_Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update fields from lookup dictionaries\n",
    "df['Reporting_Category'] = df['Reporting_Category'].map(trpa_reportingcategory_lookup)\n",
    "df['Permit_Type'] = df['Permit_Type'].map(trpa_permittype_lookup)\n",
    "df['Permit_Status'] = df['Permit_Status'].map(trpa_permitstatus_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### City of South Lake Tahoe Permit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## City of South Lake Tahoe Permit data was sent over by Ryan Malhoski on 4/9/2021\n",
    "dfCSLTPermit = read_file(\"data\\PermitData_CSLT_040924.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCSLTPermit.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Transformation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop existing 'Address' field\n",
    "df = dfCSLTPermit.drop('Address', axis=1)\n",
    "\n",
    "# final fields for all permit dataframes\n",
    "fields = ['APN', 'Address', 'Jurisdiction', \n",
    "          'Permit_ID', 'Permit_Type','Permit_Status', 'Description',\n",
    "          'Applied_Date', 'Issued_Date', 'Finaled_Date'\n",
    "          ]\n",
    "\n",
    "# # set fields\n",
    "column_mapping = {\n",
    "            'Parcel ID': 'APN',\n",
    "            'Location Address':'Address',\n",
    "            'Permit Number' : 'Permit_ID',\n",
    "            'Note Text' : 'Description',\n",
    "            'Status' : 'Permit_Status',\n",
    "            'Permit Type' : 'Permit_Type',\n",
    "            'Permit Issue Date' : 'Applied_Date',\n",
    "            'Certificate Issue Date': \"Finaled_Date\"\n",
    "            }\n",
    "\n",
    "# rename columns based on dictionary\n",
    "df = renamecolumns(df, column_mapping,False)\n",
    "\n",
    "# add missing fields\n",
    "for field in fields:\n",
    "    # if field not in dataframe add it\n",
    "    if field not in df.columns:\n",
    "        # insert new column\n",
    "        df[field] = None\n",
    "# limit to the final fields\n",
    "df = df[fields]\n",
    "# add jurisdiction value\n",
    "df.Jurisdiction = \"CSLT\"\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APN is a PPNO format in the CSLT data, and also contains EL old naming convetion (-0)\n",
    "# need to format to xxx-xxx-xxx and filter any odd values (e.g. 500 series)\n",
    "# get rid of 100's and 500's series, and format to xxx-xxx-xxx, also remove any that start with strings\n",
    "# strip off trailing spaces\n",
    "df.APN = df.APN.str.replace(' ', '') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Processing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential values for Permit Type\n",
    "# \n",
    "# get unique permit types\n",
    "for permittype in dfCSLTPermit[\"Permit Type\"].unique():\n",
    "    print(permittype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El Dorado County Permit Data\n",
    ">  there are two files, one for all TRPA files and one for all files in our geographic area, including TRPA files and EDC files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## El Dorado Permit data representing all files in our geographic area\n",
    "## exported by Ken Kasman on 4/1/2021 from their Trakit database\n",
    "dfElDoPermit = read_file(\"data\\PermitData_ElDorado_040124.csv\")\n",
    "dfElDoPermit.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Transformation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop existing 'Address' field\n",
    "df = dfElDoPermit\n",
    "\n",
    "# final fields for all permit dataframes\n",
    "fields = ['APN', 'Address', 'Jurisdiction', \n",
    "          'Permit_ID', 'Permit_Type','Permit_Status','Description',\n",
    "          'Applied_Date', 'Issued_Date', 'Finaled_Date'\n",
    "          ]\n",
    "\n",
    "# # set fields\n",
    "column_mapping = {\n",
    "            'SITE_APN' : 'APN',\n",
    "            'SITE_ADDR':'Address',\n",
    "            'Permit Number' : 'Permit_ID',\n",
    "            'DESCRIPTION' : 'Description',\n",
    "            'STATUS' : 'Permit_Status',\n",
    "            'PERMITTYPE' : 'Permit_Type',\n",
    "            'APPLIED' : 'Applied_Date',\n",
    "            'ISSUED'  : 'Issued_Date',\n",
    "            'FINALED' : \"Finaled_Date\"\n",
    "            }\n",
    "\n",
    "# rename columns based on dictionary\n",
    "df = renamecolumns(df, column_mapping, False)\n",
    "\n",
    "# add missing fields\n",
    "for field in fields:\n",
    "    # if field not in dataframe add it\n",
    "    if field not in df.columns:\n",
    "        # insert new column\n",
    "        df[field] = None\n",
    "# limit to the final fields\n",
    "df = df[fields]\n",
    "# add jurisdiction value\n",
    "df.Jurisdiction = \"EL\"\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for permittype in dfElDoPermit[\"PERMITTYPE\"].unique():\n",
    "    print(permittype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lookup dictionary\n",
    "lookupTable = read_file(\"resources/lookup_reporting_category.csv\")\n",
    "lookupTable[\"Reporting Category\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Processing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Placer County Permit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Placer Permit Data Comes in monthly via email, and gets saved to the folder below.\n",
    "## The code below will merge all the files in the folder into a single file, return a dataframe, and export to csv\n",
    "\n",
    "# folder with the CSV files\n",
    "folder_path = r\"F:\\Research and Analysis\\Local Jurisdiction MOU data collection\\Placer MOU Files\\Placer\"\n",
    "# List to hold the DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through the files in the folder and identify CSV files\n",
    "for file_name in os.listdir(folder_path):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    # Read the CSV file into a DataFrame and append to the list\n",
    "    df = pd.read_excel(file_path)\n",
    "    # Append the DataFrame to the list\n",
    "    dfs.append(df)\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "# Add today's date at the end of the file name _MMDDYY\n",
    "today = pd.Timestamp.today().strftime(\"%m%d%y\")\n",
    "# Export the final DataFrame to a CSV file\n",
    "final_df.to_csv(\"data\\PermitData_Placer_\" + today + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Placer Permit data explained above. \n",
    "dfPlacerPermit =read_file(\"data\\PermitData_Placer_040924.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPlacerPermit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPlacerPermit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Transformation***\n",
    "> hyperlink to Placer Accela record can be bulit using SERV_PROD_CODE, B1_PER_ID1, B1_PER_ID2, B1_PER_ID3\n",
    "* https://permits.placer.ca.gov/CitizenAccess/Cap/CapDetail.aspx?Module=TRPA&TabName=TRPA&capID1=16CAP&capID2=00000&capID3=0036O&agencyCode=PLACERCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lookup dictionary\n",
    "lookupTable = read_file(\"resources/PL_lookup_reporting_category.csv\")\n",
    "lookupTable[\"Reporting Category\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Processing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merege the processed dfs\n",
    "df = pd.concat([dfTRPA, dfCSLT, dfEL, dfPL], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data\\PermitData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Accounting Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Existing Development Rights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Spatial Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPATIAL JOINS ### THIS TAKES A LONG TIME ~3 HOURS ###\n",
    "\n",
    "# spatial join 2023 parcel data to all years of parcel data\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfParcel23, \"Join_23\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"WITHIN\")\n",
    "# spatial join to get Plan Area\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfPlan, \"Join_PlanArea\", \n",
    "                           \"JOIN_ONE_TO_MANY\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get District\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfDistrict, \"Join_District\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get Town Center\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfTownCenter, \"Join_TownCenter\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get Town Center Buffer\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfTCbuffer, \"Join_TownCenterBuffer\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get CSLT\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfCSLT, \"Join_CSLT\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get County\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfCounty, \"Join_County\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get TRPA Boundary\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfTRPA, \"Join_TRPA\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To Pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get results of spatial joins as spatial dataframes\n",
    "sdf_parcel_plan       = pd.DataFrame.spatial.from_featureclass(\"Join_PlanArea\", sr=sr)\n",
    "sdf_parcel_district   = pd.DataFrame.spatial.from_featureclass(\"Join_District\", sr=sr)\n",
    "sde_parcel_towncenter = pd.DataFrame.spatial.from_featureclass(\"Join_TownCenter\", sr=sr)\n",
    "sde_parcel_tcbuffer   = pd.DataFrame.spatial.from_featureclass(\"Join_TownCenterBuffer\", sr=sr)\n",
    "sde_parcel_cslt       = pd.DataFrame.spatial.from_featureclass(\"Join_CSLT\", sr=sr)\n",
    "sde_parcel_county     = pd.DataFrame.spatial.from_featureclass(\"Join_County\", sr=sr)\n",
    "sde_parcel_trpa       = pd.DataFrame.spatial.from_featureclass(\"Join_TRPA\", sr=sr)\n",
    "sde_parcel_23         = pd.DataFrame.spatial.from_featureclass(\"Join_23\", sr=sr)\n",
    "\n",
    "# pickling the dataframes\n",
    "sdf_parcel_plan.to_pickle(data_dir / \"sdf_parcel_plan.pkl\")\n",
    "sdf_parcel_district.to_pickle(data_dir / \"sdf_parcel_district.pkl\")\n",
    "sde_parcel_towncenter.to_pickle(data_dir / \"sde_parcel_towncenter.pkl\")\n",
    "sde_parcel_tcbuffer.to_pickle(data_dir / \"sde_parcel_tcbuffer.pkl\")\n",
    "sde_parcel_cslt.to_pickle(data_dir / \"sde_parcel_cslt.pkl\")\n",
    "sde_parcel_county.to_pickle(data_dir /\"sde_parcel_county.pkl\")\n",
    "sde_parcel_trpa.to_pickle(data_dir / \"sde_parcel_trpa.pkl\")\n",
    "sde_parcel_23.to_pickle(data_dir / \"sde_parcel_23.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Map Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pickled dataframes\n",
    "# get results of spatial joins as spatial dataframes\n",
    "sdf_parcel_plan       = pd.read_pickle(data_dir/\"sdf_parcel_plan.pkl\")\n",
    "sdf_parcel_district   = pd.read_pickle(data_dir/\"sdf_parcel_district.pkl\")\n",
    "sde_parcel_towncenter = pd.read_pickle(data_dir/\"sde_parcel_towncenter.pkl\")\n",
    "sde_parcel_tcbuffer   = pd.read_pickle(data_dir/\"sde_parcel_tcbuffer.pkl\")\n",
    "sde_parcel_cslt       = pd.read_pickle(data_dir/\"sde_parcel_cslt.pkl\")\n",
    "sde_parcel_county     = pd.read_pickle(data_dir/\"sde_parcel_county.pkl\")\n",
    "sde_parcel_trpa       = pd.read_pickle(data_dir/\"sde_parcel_trpa.pkl\")\n",
    "sde_parcel_23         = pd.read_pickle(data_dir/\"sde_parcel_23.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create uniqie key for spatial join APN _ YEAR\n",
    "\n",
    "for df in dfs:\n",
    "    df['APN_YEAR'] = df['APN'] + \"_\" + df['YEAR']\n",
    "    \n",
    "sdf_parcel_plan['APN_YEAR'] = sdf_parcel_plan['APN'] + \"_\" + sdf_parcel_plan['YEAR']\n",
    "sdf_parcel_district['APN_YEAR'] = sdf_parcel_district['APN'] + \"_\" + sdf_parcel_district['YEAR']\n",
    "sde_parcel_towncenter['APN_YEAR'] = sde_parcel_towncenter['APN'] + \"_\" + sde_parcel_towncenter['YEAR']\n",
    "sde_parcel_tcbuffer['APN_YEAR'] = sde_parcel_tcbuffer['APN'] + \"_\" + sde_parcel_tcbuffer['YEAR']\n",
    "sde_parcel_cslt['APN_YEAR'] = sde_parcel_cslt['APN'] + \"_\" + sde_parcel_cslt['YEAR']\n",
    "sde_parcel_county['APN_YEAR'] = sde_parcel_county['APN'] + \"_\" + sde_parcel_county['YEAR']\n",
    "sde_parcel_trpa['APN_YEAR'] = sde_parcel_trpa['APN'] + \"_\" + sde_parcel_trpa['YEAR']\n",
    "sde_parcel_23['APN_YEAR'] = sde_parcel_23['APN'] + \"_\" + sde_parcel_23['YEAR']\n",
    "sdfParcel['APN_YEAR'] = sdfParcel['APN'] + \"_\" + sdfParcel['YEAR']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the dataframes and get a list of columns for each\n",
    "for df in [sdf_parcel_plan, sdf_parcel_district, sde_parcel_towncenter, \n",
    "           sde_parcel_tcbuffer, sde_parcel_cslt, sde_parcel_county, \n",
    "           sde_parcel_trpa, sde_parcel_23]:\n",
    "    print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set up the sdfParcel dataframe with the 2023 parcel data values and then map the values from the other spatial join dataframes\n",
    "sdfParcel['JURISDICTION']      = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.JURISDICTION_1)))\n",
    "sdfParcel['COUNTY']            = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.COUNTY_1)))\n",
    "sdfParcel['COUNTY_LANDUSE_DESCRIPTION'] = sdfParcel.APN.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.COUNTY_LANDUSE_DESCRIPTION_1)))\n",
    "sdfParcel['EXISTING_LANDUSE']  = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.EXISTING_LANDUSE_1)))\n",
    "sdfParcel['OWNERSHIP_TYPE']    = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.OWNERSHIP_TYPE_1)))\n",
    "sdfParcel['YEAR_BUILT']        = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.YEAR_BUILT_1)))\n",
    "sdfParcel['PLAN_ID']           = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.PLAN_ID_1)))\n",
    "sdfParcel['PLAN_NAME']         = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.PLAN_NAME_1)))\n",
    "# sdfParcel['PLAN_TYPE']         = sdfParcel.APN.map(dict(zip(sde_parcel_23.APN, sde_parcel_23.PLAN_TYPE_1)))\n",
    "sdfParcel['ZONING_ID']         = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.ZONING_ID_1)))\n",
    "sdfParcel['ZONING_DESCRIPTION']= sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.ZONING_DESCRIPTION_1)))\n",
    "sdfParcel['TOWN_CENTER']       = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.TOWN_CENTER_1)))\n",
    "sdfParcel['LOCATION_TO_TOWNCENTER'] = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.LOCATION_TO_TOWNCENTER_1)))\n",
    "sdfParcel['TAZ']               = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.TAZ_1)))\n",
    "sdfParcel['PARCEL_ACRES']      = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.PARCEL_ACRES_1)))\n",
    "sdfParcel['PARCEL_SQFT']       = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.PARCEL_SQFT_1)))\n",
    "sdfParcel['WITHIN_BONUSUNIT_BNDY'] = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.WITHIN_BONUSUNIT_BNDY_1)))\n",
    "sdfParcel['WITHIN_TRPA_BNDY'] = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.WITHIN_TRPA_BNDY_1)))\n",
    "\n",
    "# map dictionary to sdf_units dataframe to fill in TAZ and Block Group fields\n",
    "sdfParcel['PLAN_ID']     = sdfParcel.APN_YEAR.map(dict(zip(sdf_parcel_plan.APN_YEAR, sdf_parcel_plan.PLAN_ID_1)))\n",
    "sdfParcel['PLAN_NAME']   = sdfParcel.APN_YEAR.map(dict(zip(sdf_parcel_plan.APN_YEAR, sdf_parcel_plan.PLAN_NAME_1)))\n",
    "# sdfParcel['PLAN_TYPE']   = sdfParcel.APN.map(dict(zip(sdf_parcel_plan.APN, sdf_parcel_plan.PLAN_TYPE_1)))\n",
    "\n",
    "sdfParcel['ZONING_ID']     = sdfParcel.APN_YEAR.map(dict(zip(sdf_parcel_district.APN_YEAR, sdf_parcel_district.ZONING_ID_1)))\n",
    "sdfParcel['ZONING_DESCRIPTION']   = sdfParcel.APN_YEAR.map(dict(zip(sdf_parcel_district.APN_YEAR, sdf_parcel_district.ZONING_DESCRIPTION_1))\n",
    "                                                      ) \n",
    "# using sdf_parcel_towncenter and sdf_parcel_tcbuffer\n",
    "sdfParcel['TOWN_CENTER']              = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_towncenter.APN_YEAR, sde_parcel_towncenter.Name)))\n",
    "sdfParcel['LOCATION_TO_TOWNCENTER']   = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_tcbuffer.APN_YEAR, sde_parcel_tcbuffer.BUFFER_NAME)))\n",
    "# # using sdf_parcel_cslt and sdf_parcel_county\n",
    "# sdfParcel['JURISDICTION'] = sdfParcel.APN.map(dict(zip(sdf_parcel_cslt.APN, sdf_parcel_cslt.JURISDICTION)))\n",
    "# sdfParcel['COUNTY']       = sdfParcel.APN.map(dict(zip(sdf_parcel_county.APN, sdf_parcel_county.COUNTY)))\n",
    "\n",
    "# sdfParcel['WITHIN_BONUSUNIT_BNDY'] = sdfParcel.APN.map(dict(zip(sdf_parcel_trpa.APN, sdf_parcel_trpa.WITHIN_BONUS_UNIT_BNDY)))\n",
    "# sdfParcel['WITHIN_TRPA_BNDRY'] = sdfParcel.APN.map(dict(zip(sde_parcel_trpa.APN, sde_parcel_trpa.WITHIN_TRPA_BNDY_TRPA)))\n",
    "# sdfParcel to pickle\n",
    "sdfParcel.to_pickle(data_dir/\"sdfParcel.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to feature class\n",
    "sdfParcel.spatial.to_featureclass(location=os.path.join(gdb,\"sdfParcel\"), sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total parcel count by year\n",
    "parcel_count = sdfParcel.groupby('YEAR').size()\n",
    "parcel_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfParcel.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature classes from sde\n",
    "sde_ParcelAtt    = sdeCollect+\"\\\\SDE.Parcel\\\\SDE.Parcel_History_Attributed\"\n",
    "# get data frame from feature class\n",
    "sdfParcel_SDE = pd.DataFrame.spatial.from_featureclass(sde_ParcelAtt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total Residenatial units by year\n",
    "residential_units = sdfParcel_SDE.groupby('YEAR')['Residential_Units'].sum()\n",
    "residential_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get change in residential units by year and jurisdiction\n",
    "residential_units = sdfParcel_SDE.groupby(['YEAR','ZONING_ID'])['Residential_Units'].sum()\n",
    "residential_units  = residential_units.unstack()\n",
    "# get difference between years\n",
    "differences = residential_units.diff(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences.to_csv(data_dir / \"Unit_Differences_by_Year_Zone.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to years\n",
    "sdfParcel_SDE = sdfParcel_SDE[sdfParcel_SDE.YEAR.isin([2012,2018,2019,2020,2021,2022,2023])]\n",
    "# send to pickle\n",
    "sdfParcel_SDE.to_pickle(data_dir / \"sdfParcel_SDE.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_units = sdfParcel_SDE.groupby('YEAR')['Residential_Units'].sum()\n",
    "res_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfParcel_SDE.JURISDICTION.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfParcel.JURISDICTION.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# residential_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residential_units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Edit Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS TAKES 2 HOURS TO RUN ###\n",
    "\n",
    "# feature classes from sde\n",
    "sde_ParcelAtt    = sdeCollect + \"\\\\SDE.Parcel\\\\SDE.Parcel_History_Attributed\"\n",
    "# read in staging feature class\n",
    "sdfParcelFC = os.path.join(gdb, \"sdfParcel\")\n",
    "\n",
    "# start an edit session\n",
    "edit = arcpy.da.Editor(sdeCollect)  \n",
    "edit.startEditing(False, False)\n",
    "edit.startOperation()\n",
    "\n",
    "# use field join multikey to join the data\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['JURISDICTION'], \n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['JURISDICTION'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['COUNTY'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['COUNTY'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['COUNTY_LANDUSE_DESCRIPTION'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['COUNTY_LANDUSE_DESCRIPTION'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['EXISTING_LANDUSE'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['EXISTING_LANDUSE'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['OWNERSHIP_TYPE'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['OWNERSHIP_TYPE'])    \n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['YEAR_BUILT'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['YEAR_BUILT'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['PLAN_ID'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['PLAN_ID'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['PLAN_NAME'],    \n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['PLAN_NAME'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['ZONING_ID'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['ZONING_ID'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['ZONING_DESCRIPTION'],   \n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['ZONING_DESCRIPTION'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['TOWN_CENTER'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['TOWN_CENTER'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['LOCATION_TO_TOWNCENTER'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['LOCATION_TO_TOWNCENTER'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['TAZ'], \n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['TAZ'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['PARCEL_ACRES'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['PARCEL_ACRES'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['PARCEL_SQFT'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['PARCEL_SQFT'])\n",
    "# fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['WITHIN_BONUSUNIT_BNDY'],\n",
    "#                        sdfParcelFC,   ['APN', 'YEAR'],['WITHIN_BONUSUNIT_BNDY'])\n",
    "# fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['WITHIN_TRPA_BNDY'], \n",
    "#                        sdfParcelFC,   ['APN', 'YEAR'],['WITHIN_TRPA_BNDY'])\n",
    "\n",
    "# close the edit session\n",
    "edit.stopOperation()\n",
    "edit.stopEditing(True)\n",
    "\n",
    "\n",
    "# # use field join multikey to join the data all fields! ### THIS DOESNT WORK YET ### \n",
    "# fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['JURISDICTION','COUNTY','COUNTY_LANDUSE_DESCRIPTION','EXISTING_LANDUSE',\n",
    "#                                            '            OWNERSHIP_TYPE','YEAR_BUILT','PLAN_ID','PLAN_NAME','ZONING_ID','ZONING_DESCRIPTION',\n",
    "#                                            '            TOWN_CENTER','LOCATION_TO_TOWNCENTER','TAZ','PARCEL_ACRES','PARCEL_SQFT',\n",
    "#                                                         'WITHIN_BONUSUNIT_BNDY','WITHIN_TRPA_BNDY'], \n",
    "#                        sdfParcelFC,   ['APN', 'YEAR'],['JURISDICTION','COUNTY','COUNTY_LANDUSE_DESCRIPTION','EXISTING_LANDUSE',\n",
    "#                                                         'OWNERSHIP_TYPE','YEAR_BUILT','PLAN_ID','PLAN_NAME','ZONING_ID','ZONING_DESCRIPTION',\n",
    "#                                                         'TOWN_CENTER','LOCATION_TO_TOWNCENTER','TAZ','PARCEL_ACRES','PARCEL_SQFT',\n",
    "#                                                         'WITHIN_BONUSUNIT_BNDY','WITHIN_TRPA_BNDY'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the change in development units year over year\n",
    "# group by year and county and sum residential units\n",
    "sdfParcelYear = parcel_history.groupby(['COUNTY', 'YEAR']).agg({'Residential_Units':'sum'}).reset_index()\n",
    "# sdfParcelYear['NET_CHANGE_RES'] = sdfParcel.groupby('COUNTY', 'YEAR').diff()\n",
    "sdfParcelYear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the plan name and id\n",
    "df.groupby(['PLAN_ID', 'YEAR']).agg({'Residential_Units':'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot_table(index='APN', columns='YEAR', values=['Residential_Units','CommercialFloorArea_SqFt', 'TouristAccommodation_Units'], aggfunc='sum').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# group by PLAN_NAME and sum Residential_Units\n",
    "df1 = df.groupby('PLAN_ID').agg({'Residential_Units':'sum', 'TouristAccommodation_Units':'sum','CommercialFloorArea_SqFt':'sum'}).reset_index()\n",
    "\n",
    "# print\n",
    "df1.sort_values('Residential_Units', ascending=False)\n",
    "\n",
    "# add total row\n",
    "df1.loc['Total'] = df1.sum(numeric_only=True, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export to CSV\n",
    "# df = parcel_history\n",
    "\n",
    "# columns to keep\n",
    "columns_to_keep = ['APN', 'Residential_Units', 'TouristAccommodation_Units',\n",
    "                    'CommercialFloorArea_SqFt', 'YEAR',\n",
    "                    'JURISDICTION', 'COUNTY', \n",
    "                    # 'ADU', 'RBU', 'Allocation','Deed_Restricted_Units',\n",
    "                    'OWNERSHIP_TYPE','EXISTING_LANDUSE',\n",
    "                    # 'WITHIN_TRPA_BNDY'\n",
    "                    'PARCEL_ACRES', 'PARCEL_SQFT']\n",
    "\n",
    "# add integer columns for RBU, ADU, Allocation, and Deed Restricted Units\n",
    "df['ADU'] = 0\n",
    "df['RBU'] = 0\n",
    "df['Allocation'] = 0\n",
    "df['Deed_Restricted_Units'] = 0\n",
    "\n",
    "# keep only the columns in the list\n",
    "df = df[columns_to_keep]\n",
    "# # filter to 2023\n",
    "# df = df[df.YEAR == 2023]\n",
    "\n",
    "# export to csv with date stamp in name\n",
    "today = pd.Timestamp.today().strftime(\"%m%d%y\")\n",
    "df.to_csv(\"data\\DevelopmentHistory_2023_\" + today + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get 2022 development units\n",
    "devhistoryURL = \"https://maps.trpa.org/server/rest/services/Existing_Development/MapServer/2\"\n",
    "parcel_history = get_fs_data_spatial(devhistoryURL)\n",
    "\n",
    "parcel_history.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get 2022 development units\n",
    "devhistoryURL = \"https://maps.trpa.org/server/rest/services/Existing_Development/MapServer/2\"\n",
    "parcel_history = get_fs_data_spatial(devhistoryURL)\n",
    "\n",
    "# get unit table as pandas dataframe\n",
    "unitsTable = pd.read_csv(\"data/CumulativeAccounting_2012to2023_Updated.csv\", low_memory=False)\n",
    "# get rid of columns after YEAR\n",
    "unitsTable.drop(unitsTable.columns[unitsTable.columns.get_loc(\"YEAR\")+1:], axis=1,inplace=True)\n",
    "# set cfa to numeric\n",
    "unitsTable['CommercialFloorArea_SqFt'] = pd.to_numeric(unitsTable['CommercialFloorArea_SqFt'], errors='coerce').fillna(0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplciates\n",
    "years = [2012, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "for year in years:\n",
    "    print(year)\n",
    "    # make a list of duplicate APNs\n",
    "    duplicateAPNs = sdfParcel.loc[sdfParcel['YEAR'] == year].APN[sdfParcel.loc[sdfParcel['YEAR'] == year].APN.duplicated()].tolist()\n",
    "    # print out duplicate rows\n",
    "    print(duplicateAPNs)\n",
    "    # get the number of duplicates in each list\n",
    "    print(len(duplicateAPNs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "years = [2012, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "version = \"_v6_\"\n",
    "\n",
    "# merge parcel history and units table by year and \n",
    "# export to feature class\n",
    "def merge_and_export(parcel_history, unitsTable, years):\n",
    "    for year in years:\n",
    "        print(year)\n",
    "        # filter parcel_history by year\n",
    "        parcel_history_year = parcel_history.loc[parcel_history['YEAR'] == year]\n",
    "        # filter unitsTable by year\n",
    "        unitsTable_year = unitsTable.loc[unitsTable['YEAR'] == year]\n",
    "        # merge parcel_history_year and unitsTable_year\n",
    "        df = pd.merge(parcel_history_year, unitsTable_year, on='APN', how='left', indicator=True)\n",
    "        # make sure field types are numeric for Residential_Unit, TouristAccommodation_Units, and CommercialFloorArea_SqFt fields\n",
    "        df['Residential_Units']          = pd.to_numeric(df['Residential_Units_y'], errors='coerce')\n",
    "        df['TouristAccommodation_Units'] = pd.to_numeric(df['TouristAccommodation_Units_y'], errors='coerce')\n",
    "        df['CommercialFloorArea_SqFt']   = pd.to_numeric(df['CommercialFloorArea_SqFt_y'], errors='coerce')\n",
    "        # if NaN in Residential_Units, set to 0\n",
    "        df['Residential_Units'] = df['Residential_Units'].fillna(0)\n",
    "        # if NaN in TouristAccommodation_Units, set to 0\n",
    "        df['TouristAccommodation_Units'] = df['TouristAccommodation_Units'].fillna(0)\n",
    "        # if NaN in CommercialFloorArea_SqFt, set to 0\n",
    "        df['CommercialFloorArea_SqFt'] = df['CommercialFloorArea_SqFt'].fillna(0)\n",
    "        # change YEAR_y to YEAR\n",
    "        df['YEAR'] = df['YEAR_y']\n",
    "        # Sanitize column names\n",
    "        df.columns = [re.sub(r'[^a-zA-Z0-9_]', '_', col) for col in df.columns]\n",
    "        # set output feature class name\n",
    "        yearstr = str(year)\n",
    "        outfc = f\"Parcel_History_Attributed{version}{yearstr}\"    \n",
    "        # export updated parcel history to feature class filtered by year\n",
    "        df.spatial.to_featureclass(location=os.path.join(\"C:/GIS/Scratch.gdb\", outfc), overwrite=True, sanitize_columns=False)\n",
    "\n",
    "# identify parcels that did not join from the merge\n",
    "def get_unjoined(parcel_history, unitsTable, years):\n",
    "    for year in years:\n",
    "        # Filter parcel_history for the current year\n",
    "        parcel_history_filtered = parcel_history.loc[parcel_history['YEAR'] == year]\n",
    "        \n",
    "        # Merge with unitsTable for the same year\n",
    "        units_by_year = unitsTable.loc[unitsTable.YEAR == year]\n",
    "        merged_data = units_by_year.merge(parcel_history_filtered, on='APN', how='outer', indicator=True)\n",
    "        \n",
    "        # Print year and merge value counts\n",
    "        print(year)\n",
    "        print(merged_data._merge.value_counts())\n",
    "        \n",
    "        # Data manipulations\n",
    "        merged_data = merged_data.rename(columns={'YEAR_x': 'YEAR'})\n",
    "        merged_data = merged_data.loc[merged_data._merge != 'both']\n",
    "        merged_data.info()\n",
    "        merged_data = merged_data[['APN', 'YEAR', '_merge', 'CommercialFloorArea_SqFt_x', 'Residential_Units_x', 'TouristAccommodation_Units_x',\n",
    "                                   'CommercialFloorArea_SqFt_y', 'Residential_Units_y', 'TouristAccommodation_Units_y']]\n",
    "        merged_data.info()\n",
    "        # Save to CSV\n",
    "        merged_data.to_csv(f\"data\\\\Parcel_History_Attributed_APN_Merge{version, year}.csv\", index=False)\n",
    "\n",
    "# check for duplicates in parcel_history\n",
    "def check_duplicates(parcel_history, unitsTable, years):\n",
    "    for year in years:\n",
    "        print(year)\n",
    "        # make a list of duplicate APNs\n",
    "        duplicateAPNs = parcel_history.loc[parcel_history['YEAR'] == year].APN[parcel_history.loc[parcel_history['YEAR'] == year].APN.duplicated()].tolist()\n",
    "        # print out duplicate rows\n",
    "        print(duplicateAPNs)\n",
    "        # save 2021 duplicates to csv\n",
    "        if year == 2018:\n",
    "            parcel_history.loc[parcel_history['YEAR'] == year].loc[parcel_history.loc[parcel_history['YEAR'] == year].APN.duplicated()].to_csv(\"data\\Parcel_History_Duplicates_2018.csv\")\n",
    "\n",
    "        # make a list of duplicate APNs in unitsTable\n",
    "        duplicateAPNsCA = unitsTable.loc[unitsTable['YEAR'] == year].APN[unitsTable.loc[unitsTable['YEAR'] == year].APN.duplicated()].tolist()\n",
    "        # print out duplicate rows\n",
    "        print(duplicateAPNsCA)\n",
    "\n",
    "# compare total Residnetial Units, Commercial Floor Area, and Tourist Accommodation Units by year, bewtween parcel_history and unitsTable\n",
    "def compare_totals(parcel_history, unitsTable, years):\n",
    "    for year in years:\n",
    "        # filter parcel_history by year\n",
    "        parcel_history_year = parcel_history.loc[parcel_history['YEAR'] == year]\n",
    "        # filter unitsTable by year\n",
    "        unitsTable_year = unitsTable.loc[unitsTable['YEAR'] == year]\n",
    "        # # remove any commas from CommercialFloorArea_SqFt in unitsTable_year using .loc\n",
    "        # unitsTable_year.loc[:, 'CommercialFloorArea_SqFt'] = unitsTable_year['CommercialFloorArea_SqFt'].str.replace(',', '').astype(float)\n",
    "\n",
    "        # get sum of Residential Units in parcel_history\n",
    "        resTotal = parcel_history_year['Residential_Units'].sum()\n",
    "        cfaTotal = parcel_history_year['CommercialFloorArea_SqFt'].sum()\n",
    "        tauTotal = parcel_history_year['TouristAccommodation_Units'].sum()\n",
    "\n",
    "        # get sum of Residential Units in unitsTable\n",
    "        resTotalCA = unitsTable_year['Residential_Units'].sum()\n",
    "        cfaTotalCA = unitsTable_year['CommercialFloorArea_SqFt'].sum()\n",
    "        tauTotalCA = unitsTable_year['TouristAccommodation_Units'].sum()\n",
    "\n",
    "        # print totals\n",
    "        print(year)\n",
    "        print('Residential Units in Parcel_History \\n' + str(resTotal))\n",
    "        print('Residential Units in updated table \\n'+ str(resTotalCA))\n",
    "        print('Commercial Floor Area in Parcel_History \\n'+ str(cfaTotal))\n",
    "        print('Commercial Floor Area in updated table \\n'+ str(cfaTotalCA))\n",
    "        print('Tourist Accommodation Units in Parcel_History \\n'+ str(tauTotal))\n",
    "        print('Tourist Accommodation Units in updated table \\n'+ str(tauTotalCA))\n",
    "\n",
    "# identify rows where the Residential Units, Commercial Floor Area, and Tourist Accommodation Units are different between parcel_history and unitsTable\n",
    "def find_different_rows(parcel_history, unitsTable, years):\n",
    "    for year in years:\n",
    "        print(year)\n",
    "        # filter parcel_history by year\n",
    "        parcel_history_year = parcel_history.loc[parcel_history['YEAR'] == year]\n",
    "        # filter unitsTable by year\n",
    "        unitsTable_year = unitsTable.loc[unitsTable['YEAR'] == year]\n",
    "        # # remove any commas from CommercialFloorArea_SqFt in unitsTable_year using .loc\n",
    "        # unitsTable_year.loc[:, 'CommercialFloorArea_SqFt'] = unitsTable_year['CommercialFloorArea_SqFt'].str.replace(',', '').astype(float)\n",
    "        # merge parcel_history_year and unitsTable_year\n",
    "        df = pd.merge(parcel_history_year, unitsTable_year, right_on='APN', left_on='APN', how='outer', indicator=True)\n",
    "        # drop columns that are not needed\n",
    "        df = df[['APN', 'YEAR_x','YEAR_y', 'Residential_Units_x', 'CommercialFloorArea_SqFt_x', 'TouristAccommodation_Units_x', 'Residential_Units_y', 'CommercialFloorArea_SqFt_y', 'TouristAccommodation_Units_y']]\n",
    "        # get fields where the Residential Units, Commercial Floor Area, and Tourist Accommodation Units do not match\n",
    "        df = df.loc[(df['Residential_Units_x'] != df['Residential_Units_y']) | (df['CommercialFloorArea_SqFt_x'] != df['CommercialFloorArea_SqFt_y']) | (df['TouristAccommodation_Units_x'] != df['TouristAccommodation_Units_y'])]\n",
    "        # print out the rows\n",
    "        print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(parcel_history, unitsTable, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the merge functions to export feature classes and get unjoined data as csv\n",
    "# merge_and_export(parcel_history, unitsTable, years)\n",
    "get_unjoined(parcel_history, unitsTable, years)\n",
    "check_duplicates(parcel_history, unitsTable, years)\n",
    "compare_totals(parcel_history, unitsTable, years)\n",
    "find_different_rows(parcel_history, unitsTable, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze the changes in parcel history by year\n",
    "years = [2012, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "df = sdfUnits\n",
    "for year in years:\n",
    "    print(year)\n",
    "    # filter parcel_history by year\n",
    "    parcel_history_year = df.loc[df['YEAR'] == year]\n",
    "    # get sum of Residential Units in parcel_history\n",
    "    resTotal = parcel_history_year['Residential_Units'].sum()\n",
    "    cfaTotal = parcel_history_year['CommercialFloorArea_SqFt'].sum()\n",
    "    tauTotal = parcel_history_year['TouristAccommodation_Units'].sum()\n",
    "    # print totals\n",
    "    print('Residential Units in Parcel_History \\n' + str(resTotal))\n",
    "    print('Commercial Floor Area in Parcel_History \\n'+ str(cfaTotal))\n",
    "    print('Tourist Accommodation Units in Parcel_History \\n'+ str(tauTotal))\n",
    "    # print out changes in units by APN\n",
    "# firnd all the rows where duplicate APNs change units between years\n",
    "for year in years:\n",
    "    print(year)\n",
    "    # make a list of duplicate APNs as sets of APNs\n",
    "    duplicateAPNs = df.loc[df['YEAR'] == year].APN[df.loc[df['YEAR'] == year].APN.duplicated()].tolist()\n",
    "    # loop through the duplicate APNs\n",
    "    for apn in duplicateAPNs:\n",
    "        # get the rows for the APN\n",
    "        df = df.loc[df['APN'] == apn]\n",
    "        # get the rows for the APN by year\n",
    "        df = df.loc[df['YEAR'] == year]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total residential units by year\n",
    "def get_totals(parcels, years):\n",
    "    # total\n",
    "    total = pd.DataFrame(columns=['Year', 'Residential_Units'])\n",
    "    for year in years:\n",
    "        # filter parcel_history by year\n",
    "        parcel_history_year = parcels.loc[parcels['YEAR'] == year]\n",
    "        # get sum of Residential Units in parcel_history\n",
    "        resTotal = parcel_history_year['Residential_Units'].sum()\n",
    "\n",
    "        # add new row using concat\n",
    "        total = pd.concat([total, pd.DataFrame({'Year': [year], 'Residential_Units': [resTotal]})])\n",
    "    return total\n",
    "\n",
    "# get total residential units by year\n",
    "total = get_totals(parcel_history, years)\n",
    "# calculate percentage change in residential units year over year\n",
    "total['Percent_Change'] = (total['Residential_Units'].pct_change())*100\n",
    "# create a new column for the difference in residential units year over year\n",
    "total['Difference'] = total['Residential_Units'].diff()\n",
    "\n",
    "total\n",
    "# export to csv\n",
    "total.to_csv('total_residential_units_by_year.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Transformation***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Proecssing***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deed Restrictions\n",
    "> Deed restricted unit research needs to be merged with LTinfo housing deed restricitons and parcel unit data from 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedUnits  = read_excel(\"data\\Housing_Deed_Restrcitions.xlsx\", 0)\n",
    "dfDeedLTinfo = pd.read_json(\"https://laketahoeinfo.org/WebServices/GetDeedRestrictedParcels/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedUnits.to_csv(\"data\\DeedRestricted_HousingUnits.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedUnits.Units.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedLTinfo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique values for deed restrcition type\n",
    "dfDeedLTinfo.DeedRestrictionType.unique()\n",
    "\n",
    "# filter to Affordable, Achievable, and Moderate\n",
    "dfDeedLTinfo = dfDeedLTinfo[dfDeedLTinfo.DeedRestrictionType.isin(['Affordable Housing', 'Moderate Income Housing', 'Achievable Housing'])]  \n",
    "\n",
    "# count of total records\n",
    "dfDeedLTinfo.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcelUnits22.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedUnitsMerge = dfDeedUnits.merge(dfDeedLTinfo, on='APN', how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedUnitsMerge._merge.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedLTinfo[dfDeedLTinfo.duplicated(subset=['APN','DeedRestrictionType'], keep=False)].sort_values('APN').to_csv(\"HousingDeedRestrictions_LTinfo_Duplicates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify duplicates unique by APN and \n",
    "dfDeedUnits[dfDeedUnits.duplicated(subset=['APN', 'Deed_Restriction_Type','Units'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify duplicates\n",
    "dfDeedUnitsMerge[dfDeedUnitsMerge.duplicated(subset=['APN'], keep=False)].sort_values(by='APN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedUnitsMerge.to_csv(\"HousingDeedRestrictions_All.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the deed restricted units with the parcel units\n",
    "dfDeedUnits_ParcelUnits  = dfDeedUnits.merge(parcelUnits22, on='APN', how='left')\n",
    "# merge the deed restricted units with the parcel units\n",
    "dfDeedLTinfo_ParcelUnits = dfDeedLTinfo.merge(parcelUnits22, left_on='APN', right_on='APN', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedLTinfo_ParcelUnits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedLTinfo_ParcelUnits.Residential_Units.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADU Tracking\n",
    "> ADU permit tracking from TRPA and othe Jurisdictions. There is a need to establish a system of record for this information (LT Info). This is similar to the Residential Bonus Unit data and theres crossover on some of these, where a bonus unit was used to create an ADU, but you can have an ADU without requiring a bonus unit, and you can use a bonus unit without it being an ADU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfADU = read_excel(\"data\\ADU Tracking.xlsx\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfADU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Allocations\n",
    "> This file includes all of the allocations that have been tracked in LT Info, and adds in whether the subject parcel has been issued a BMP/SCC certificate and/or whether Air Quality/Mobility Mitigation fees (for added VMT) or Water Quality Mitigation fees (for added coverage) have been paid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allocations = read_excel(\"data\\Allocation_Tracking.xlsx\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transactions with Inactive APNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inactiveParcels = read_file(\"data\\Transactions_InactiveParcels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Process to compare against assessor parcel data signifying development\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parcels feature class of missing parcels for Residential Units\n",
    "\n",
    "# get parcel master\n",
    "parcelURL = \"https://maps.trpa.org/server/rest/services/Parcels/MapServer/0\"\n",
    "vhrURL    = \"https://maps.trpa.org/server/rest/services/VHR/MapServer/0\"\n",
    "# get parcel and VHR data as spatial dataframes\n",
    "sdfParcel = get_fs_data_spatial(parcelURL)\n",
    "# sdfVHR    = get_fs_data_spatial(vhrURL)\n",
    "\n",
    "# # keep only the columns needed\n",
    "# sdfParcel = sdfParcel[['APN','EXISTING_LANDUSE','YEAR_BUILT','BEDROOMS','UNITS','SHAPE']]\n",
    "# sdfVHR    = sdfVHR[['APN','SHAPE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets feature service data as spatially enabled dataframe\n",
    "def get_fs_data_spatial(service_url):\n",
    "    feature_layer = FeatureLayer(service_url)\n",
    "    df = feature_layer.query().sdf\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the parcel and VHR data\n",
    "sdf = pd.merge(sdfParcel, sdfVHR, on='APN', how='left', indicator=True)\n",
    "# merge the 2023 parcelhistory and \n",
    "parcelDev2023 = parcel_history.loc[parcel_history['YEAR'] == 2023]\n",
    "sdf = pd.merge(sdf, parcelDev2023, on='APN', how='left', indicator=True)\n",
    "sdf.info()\n",
    "# # keep fields needed for QA\n",
    "# sdf = sdf[['APN','EXISTING_LANDUSE','YEAR_BUILT','BEDROOMS','UNITS','WITHIN_TRPABNDY','_merge','SHAPE']]\n",
    "# # export to feature class\n",
    "# sdf.spatial.to_featureclass(location=os.path.join(arcpy.env.workspace, 'Parcel_Review'), overwrite=True, sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
