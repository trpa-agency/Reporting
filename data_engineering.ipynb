{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TO DO\n",
    "* Add ADU, Bonus Unit, and Allocation fields to Parcel_History_Attributed\n",
    "* Clean up unnecessary fields in Parcel_History_Attributed\n",
    "* Add VHR/Bedrooms to the data\n",
    "* Add CFA research from Ken to the data\n",
    "* Check totals year over year\n",
    "* Add in way to model TAUs as Residential units where City converted Hotels>Apartments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terminology "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data engineering can consist of ***collection, cleaning, transformation, processing, and automating and monitoring tasks***\n",
    "* Collection - examples include getting data from a rest service as a\n",
    "* Cleaning - categorizing \n",
    "* Transformation - cateogorizing, standardization, \n",
    "* Processing - algorithm, pivot, groupby, merge\n",
    "* Automating - schedule task, Apache Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Planning Jargon\n",
    "* ADU - Accessory Dwelling Unit\n",
    "* Existing Development Right - refers to residential, commercial, or tourist development currently built in the Lake Tahoe Basin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import arcpy\n",
    "from arcgis.features import FeatureLayer, GeoAccessor, GeoSeriesAccessor\n",
    "from arcgis.gis import GIS\n",
    "from utils import *\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "import datetime\n",
    "from time import strftime  \n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge parcel history and units table by year and \n",
    "# export to feature class\n",
    "def merge_and_export(parcel_history, unitsTable, years):\n",
    "    for year in years:\n",
    "        print(year)\n",
    "        # filter parcel_history by year\n",
    "        parcel_history_year = parcel_history.loc[parcel_history['YEAR'] == year]\n",
    "        # filter unitsTable by year\n",
    "        unitsTable_year = unitsTable.loc[unitsTable['YEAR'] == year]\n",
    "        # merge parcel_history_year and unitsTable_year\n",
    "        df = pd.merge(parcel_history_year, unitsTable_year, on='APN', how='left', indicator=True)\n",
    "        # make sure field types are numeric for Residential_Unit, TouristAccommodation_Units, and CommercialFloorArea_SqFt fields\n",
    "        df['Residential_Units']          = pd.to_numeric(df['Residential_Units_y'], errors='coerce')\n",
    "        df['TouristAccommodation_Units'] = pd.to_numeric(df['TouristAccommodation_Units_y'], errors='coerce')\n",
    "        df['CommercialFloorArea_SqFt']   = pd.to_numeric(df['CommercialFloorArea_SqFt_y'], errors='coerce')\n",
    "        # if NaN in Residential_Units, set to 0\n",
    "        df['Residential_Units'] = df['Residential_Units'].fillna(0)\n",
    "        # if NaN in TouristAccommodation_Units, set to 0\n",
    "        df['TouristAccommodation_Units'] = df['TouristAccommodation_Units'].fillna(0)\n",
    "        # if NaN in CommercialFloorArea_SqFt, set to 0\n",
    "        df['CommercialFloorArea_SqFt'] = df['CommercialFloorArea_SqFt'].fillna(0)\n",
    "        # change YEAR_y to YEAR\n",
    "        df['YEAR'] = df['YEAR_y']\n",
    "        # Sanitize column names\n",
    "        df.columns = [re.sub(r'[^a-zA-Z0-9_]', '_', col) for col in df.columns]\n",
    "        # set output feature class name\n",
    "        yearstr = str(year)\n",
    "        outfc = f\"Parcel_History_Attributed{version}{yearstr}\"    \n",
    "        # export updated parcel history to feature class filtered by year\n",
    "        df.spatial.to_featureclass(location=os.path.join(\"C:/GIS/Scratch.gdb\", outfc), overwrite=True, sanitize_columns=False)\n",
    "\n",
    "# identify parcels that did not join from the merge\n",
    "def get_unjoined(parcel_history, unitsTable, years):\n",
    "    for year in years:\n",
    "        # Filter parcel_history for the current year\n",
    "        parcel_history_filtered = parcel_history.loc[parcel_history['YEAR'] == year]\n",
    "        \n",
    "        # Merge with unitsTable for the same year\n",
    "        units_by_year = unitsTable.loc[unitsTable.YEAR == year]\n",
    "        merged_data = units_by_year.merge(parcel_history_filtered, on='APN', how='outer', indicator=True)\n",
    "        \n",
    "        # Print year and merge value counts\n",
    "        print(year)\n",
    "        print(merged_data._merge.value_counts())\n",
    "        \n",
    "        # Data manipulations\n",
    "        merged_data = merged_data.rename(columns={'YEAR_x': 'YEAR'})\n",
    "        merged_data = merged_data.loc[merged_data._merge != 'both']\n",
    "        merged_data.info()\n",
    "        merged_data = merged_data[['APN', 'YEAR', '_merge', 'CommercialFloorArea_SqFt_x', 'Residential_Units_x', 'TouristAccommodation_Units_x',\n",
    "                                   'CommercialFloorArea_SqFt_y', 'Residential_Units_y', 'TouristAccommodation_Units_y']]\n",
    "        merged_data.info()\n",
    "        # Save to CSV\n",
    "        merged_data.to_csv(f\"data\\\\Parcel_History_Attributed_APN_Merge{version, year}.csv\", index=False)\n",
    "\n",
    "# check for duplicates in parcel_history\n",
    "def check_duplicates(parcel_history, unitsTable, years):\n",
    "    for year in years:\n",
    "        print(year)\n",
    "        # make a list of duplicate APNs\n",
    "        duplicateAPNs = parcel_history.loc[parcel_history['YEAR'] == year].APN[parcel_history.loc[parcel_history['YEAR'] == year].APN.duplicated()].tolist()\n",
    "        # print out duplicate rows\n",
    "        print(duplicateAPNs)\n",
    "        # save 2021 duplicates to csv\n",
    "        if year == 2018:\n",
    "            parcel_history.loc[parcel_history['YEAR'] == year].loc[parcel_history.loc[parcel_history['YEAR'] == year].APN.duplicated()].to_csv(\"data\\Parcel_History_Duplicates_2018.csv\")\n",
    "\n",
    "        # make a list of duplicate APNs in unitsTable\n",
    "        duplicateAPNsCA = unitsTable.loc[unitsTable['YEAR'] == year].APN[unitsTable.loc[unitsTable['YEAR'] == year].APN.duplicated()].tolist()\n",
    "        # print out duplicate rows\n",
    "        print(duplicateAPNsCA)\n",
    "\n",
    "# compare total Residnetial Units, Commercial Floor Area, and Tourist Accommodation Units by year, bewtween parcel_history and unitsTable\n",
    "def compare_totals(parcel_history, unitsTable, years):\n",
    "    for year in years:\n",
    "        # filter parcel_history by year\n",
    "        parcel_history_year = parcel_history.loc[parcel_history['YEAR'] == year]\n",
    "        # filter unitsTable by year\n",
    "        unitsTable_year = unitsTable.loc[unitsTable['YEAR'] == year]\n",
    "        # # remove any commas from CommercialFloorArea_SqFt in unitsTable_year using .loc\n",
    "        # unitsTable_year.loc[:, 'CommercialFloorArea_SqFt'] = unitsTable_year['CommercialFloorArea_SqFt'].str.replace(',', '').astype(float)\n",
    "\n",
    "        # get sum of Residential Units in parcel_history\n",
    "        resTotal = parcel_history_year['Residential_Units'].sum()\n",
    "        cfaTotal = parcel_history_year['CommercialFloorArea_SqFt'].sum()\n",
    "        tauTotal = parcel_history_year['TouristAccommodation_Units'].sum()\n",
    "\n",
    "        # get sum of Residential Units in unitsTable\n",
    "        resTotalCA = unitsTable_year['Residential_Units'].sum()\n",
    "        cfaTotalCA = unitsTable_year['CommercialFloorArea_SqFt'].sum()\n",
    "        tauTotalCA = unitsTable_year['TouristAccommodation_Units'].sum()\n",
    "\n",
    "        # print totals\n",
    "        print(year)\n",
    "        print('Residential Units in Parcel_History \\n' + str(resTotal))\n",
    "        print('Residential Units in updated table \\n'+ str(resTotalCA))\n",
    "        print('Commercial Floor Area in Parcel_History \\n'+ str(cfaTotal))\n",
    "        print('Commercial Floor Area in updated table \\n'+ str(cfaTotalCA))\n",
    "        print('Tourist Accommodation Units in Parcel_History \\n'+ str(tauTotal))\n",
    "        print('Tourist Accommodation Units in updated table \\n'+ str(tauTotalCA))\n",
    "\n",
    "# identify rows where the Residential Units, Commercial Floor Area, and Tourist Accommodation Units are different between parcel_history and unitsTable\n",
    "def find_different_rows(parcel_history, unitsTable, years):\n",
    "    for year in years:\n",
    "        print(year)\n",
    "        # filter parcel_history by year\n",
    "        parcel_history_year = parcel_history.loc[parcel_history['YEAR'] == year]\n",
    "        # filter unitsTable by year\n",
    "        unitsTable_year = unitsTable.loc[unitsTable['YEAR'] == year]\n",
    "        # # remove any commas from CommercialFloorArea_SqFt in unitsTable_year using .loc\n",
    "        # unitsTable_year.loc[:, 'CommercialFloorArea_SqFt'] = unitsTable_year['CommercialFloorArea_SqFt'].str.replace(',', '').astype(float)\n",
    "        # merge parcel_history_year and unitsTable_year\n",
    "        df = pd.merge(parcel_history_year, unitsTable_year, right_on='APN', left_on='APN', how='outer', indicator=True)\n",
    "        # drop columns that are not needed\n",
    "        df = df[['APN', 'YEAR_x','YEAR_y', 'Residential_Units_x', 'CommercialFloorArea_SqFt_x', 'TouristAccommodation_Units_x', 'Residential_Units_y', 'CommercialFloorArea_SqFt_y', 'TouristAccommodation_Units_y']]\n",
    "        # get fields where the Residential Units, Commercial Floor Area, and Tourist Accommodation Units do not match\n",
    "        df = df.loc[(df['Residential_Units_x'] != df['Residential_Units_y']) | (df['CommercialFloorArea_SqFt_x'] != df['CommercialFloorArea_SqFt_y']) | (df['TouristAccommodation_Units_x'] != df['TouristAccommodation_Units_y'])]\n",
    "        # print out the rows\n",
    "        print(df)\n",
    "\n",
    "# get total residential units by year\n",
    "def get_totals(parcels, years):\n",
    "    # total\n",
    "    total = pd.DataFrame(columns=['Year', 'Residential_Units'])\n",
    "    for year in years:\n",
    "        # filter parcel_history by year\n",
    "        parcel_history_year = parcels.loc[parcels['YEAR'] == year]\n",
    "        # get sum of Residential Units in parcel_history\n",
    "        resTotal = parcel_history_year['Residential_Units'].sum()\n",
    "\n",
    "        # add new row using concat\n",
    "        total = pd.concat([total, pd.DataFrame({'Year': [year], 'Residential_Units': [resTotal]})])\n",
    "    return total\n",
    "\n",
    "def fieldJoinCalc_multikey(updateFC, updateFieldsList_key, updateFieldsList_value, sourceFC, sourceFieldsList_key, sourceFieldsList_value):\n",
    "    from time import strftime  \n",
    "    print (\"Started data transfer: \" + strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    # Use list comprehension to build a dictionary from arcpy SearchCursor  \n",
    "    total_count=0\n",
    "    valueDict = {(r[0]+str(r[1])):(r[2]) for r in arcpy.da.SearchCursor(sourceFC, (sourceFieldsList_key + sourceFieldsList_value)) if r[0] is not None and r[1] is not None}  \n",
    "    with arcpy.da.UpdateCursor(updateFC, (updateFieldsList_key + updateFieldsList_value)) as updateRows:  \n",
    "        for updateRow in updateRows:  \n",
    "            # store the Join value of the row being updated in a keyValue variable  \n",
    "            if updateRow[0] is not None and updateRow[1] is not None:\n",
    "                keyValue = updateRow[0]+str(updateRow[1])\n",
    "                # verify that the keyValue is in the Dictionary  \n",
    "                if keyValue in valueDict:\n",
    "                    total_count +=1\n",
    "                    if (total_count%1000)==0:\n",
    "                        print (f\"Updating row {total_count}\")\n",
    "                    # transfer the value stored under the keyValue from the dictionary to the updated field.  \n",
    "                    updateRow[2] = valueDict[keyValue]  \n",
    "                    updateRows.updateRow(updateRow)    \n",
    "    del valueDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data frame display options\n",
    "# pandas options\n",
    "pd.options.mode.copy_on_write = True\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows    = 999\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "   \n",
    "# set environement workspace to in memory \n",
    "arcpy.env.workspace = 'memory'\n",
    "# overwrite true\n",
    "arcpy.env.overwriteOutput = True\n",
    "# Set spatial reference to NAD 1983 UTM Zone 10N\n",
    "sr = arcpy.SpatialReference(26910)\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "# # Set the extent environment using a feature class\n",
    "# arcpy.env.extent = \"TRPA_Boundary\"\n",
    "\n",
    "# current working directory\n",
    "local_path = pathlib.Path().absolute()\n",
    "# set data path as a subfolder of the current working directory TravelDemandModel\\2022\\\n",
    "data_dir   = local_path.parents[0] / 'Reporting/data/raw_data'\n",
    "# folder to save processed data\n",
    "out_dir    = local_path.parents[0] / 'Reporting/data/processed_data'\n",
    "# workspace gdb for stuff that doesnt work in memory\n",
    "gdb        = local_path.parents[0] / 'Reporting/data/Workspace.gdb'\n",
    "\n",
    "# network path to connection files\n",
    "filePath = \"F:/GIS/PARCELUPDATE/Workspace/\"\n",
    "# database file path \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "sdeTabular = os.path.join(filePath, \"Tabular.sde\")\n",
    "sdeEdit    = os.path.join(filePath, \"Edit.sde\")\n",
    "\n",
    "# global variables\n",
    "years = [2012, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "version = \"_v6_\"\n",
    "\n",
    "# get todays date as mmddyyyy\n",
    "today = datetime.datetime.now().strftime(\"%m%d%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(today)\n",
    "print(local_path)\n",
    "print(data_dir)\n",
    "print(out_dir)\n",
    "print(gdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the GIS object\n",
    "## portal URL = \"https://maps.trpa.org/portal/home/\"\n",
    "## AGOL URL   = \"https://www.arcgis.com\"\n",
    "gis = GIS(\n",
    "    url=\"https://maps.trpa.org/portal/home/\",\n",
    "    ## enter username above ##\n",
    "    username= input(\"Enter username:\"),\n",
    "    ## enter password above ##\n",
    "    password=getpass.getpass(\"Enter password:\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a map object\n",
    "map = gis.map(\"Lake Tahoe\", zoomlevel=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sources\n",
    "* https://www.laketahoeinfo.org/WebServices/List\n",
    "* https://maps.trpa.org/server/rest/services/\n",
    "* sdeBase, sdeCollect, sdeTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature classes from sde\n",
    "sde_ParcelAtt    = os.path.join(sdeCollect, \"\\\\SDE.Parcel\\\\SDE.Parcel_History_Attributed\")\n",
    "sde_ParcelMaster = os.path.join(sdeBase,\"\\\\sde.SDE.Parcels\\\\sde.SDE.Parcel_Master\")\n",
    "\n",
    "## get parcel data\n",
    "sdfParcel     = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Existing_Development/MapServer/2\")\n",
    "sdfParcel23   = get_fs_data_spatial_query(\"https://maps.trpa.org/server/rest/services/Existing_Development/MapServer/2\", \"YEAR = 2023\") \n",
    "# get spatial data to join to\n",
    "sdfDistrict   = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Planning/MapServer/1\")\n",
    "sdfPlan       = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/MapServer/0\")\n",
    "sdfTownCenter = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/MapServer/1\")\n",
    "sdfTCbuffer   = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Planning/MapServer/4\")\n",
    "sdfCSLT       = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/MapServer/2\")\n",
    "sdfCounty     = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/MapServer/3\")\n",
    "sdfTRPA       = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/MapServer/4\")\n",
    "\n",
    "# set spatial reference\n",
    "sdfParcel.spatial.sr = sr\n",
    "sdfParcel23.spatial.sr = sr\n",
    "sdfDistrict.spatial.sr = sr\n",
    "sdfPlan.spatial.sr = sr\n",
    "sdfTownCenter.spatial.sr = sr\n",
    "sdfTCbuffer.spatial.sr = sr\n",
    "sdfCSLT.spatial.sr = sr\n",
    "sdfCounty.spatial.sr = sr\n",
    "sdfTRPA.spatial.sr = sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature classes from sde\n",
    "sde_ParcelAtt        = sdeCollect + \"\\\\SDE.Parcel\\\\SDE.Parcel_History_Attributed\"\n",
    "sde_LocalPlan        = sdeBase + \"\\\\sde.SDE.Planning\\\\sde.SDE.LocalPlan\"\n",
    "sde_CSLT             = sdeBase + \"\\\\sde.SDE.Jurisdictions\\\\sde.SDE.CSLT\"\n",
    "sde_CurrentParcels   = sdeBase + \"\\\\sde.SDE.Parcels\\\\sde.SDE.Parcel_Master\"\n",
    "sde_District         = sdeBase + \"\\\\sde.SDE.Planning\\\\sde.SDE.District\"\n",
    "sde_TownCenter       = sdeBase + \"\\\\sde.SDE.Planning\\\\sde.SDE.TownCenter\"\n",
    "sde_TownCenterBuffer = sdeBase + \"\\\\sde.SDE.Planning\\\\sde.SDE.TownCenter_Buffer\"\n",
    "sde_TRPAboundary     = sdeBase + \"\\\\sde.SDE.Jurisdictions\\\\sde.SDE.TRPA_bdy\"\n",
    "sde_BonusUnitboundary= sdeBase + \"\\\\sde.SDE.Planning\\\\sde.SDE.Bonus_unit_boundary\"\n",
    "sde_UrbanArea        = sdeBase + \"\\\\sde.SDE.Jurisdictions\\\\sde.SDE.UrbanAreas\"\n",
    "sde_Zip              = sdeBase + \"\\\\sde.SDE.Jurisdictions\\\\sde.SDE.Postal_ZIP\"\n",
    "sde_TAZ              = sdeBase + \"\\\\sde.SDE.Transportation\\\\sde.SDE.Transportation_Analysis_Zone\"\n",
    "sdf_County           = sdeBase + \"\\\\sde.SDE.Jurisdictions\\\\SDE.Counties\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LT Info Data\n",
    "# Verified Development Rights\n",
    "dfDevRight  = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetParcelDevelopmentRightsForAccela/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "# Deed Restrictions as a DataFrame\n",
    "dfDeed      = pd.read_json(\"https://laketahoeinfo.org/WebServices/GetDeedRestrictedParcels/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "# IPES LTinfo as a DataFrame\n",
    "dfIPES      = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetParcelIPESScores/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "# Development Rights Transacted and Banked as a DataFrame\n",
    "dfDevRights = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetTransactedAndBankedDevelopmentRights/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "# All Parcels as a DataFrame\n",
    "dfLTParcel  = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetAllParcels/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "\n",
    "# get development rights for Accela\n",
    "dfLTDevRightAcc = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetParcelDevelopmentRightsForAccela/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get 2022 development units\n",
    "devhistoryURL = \"https://maps.trpa.org/server/rest/services/Existing_Development/MapServer/2\"\n",
    "# get parcel history for 2023\n",
    "# df23 = get_fs_data_query(devhistoryURL, \"Year = 2023\")\n",
    "# df22 = get_fs_data_query(devhistoryURL, \"Year = 2022\")\n",
    "# df21 = get_fs_data_query(devhistoryURL, \"Year = 2021\")\n",
    "# df20 = get_fs_data_query(devhistoryURL, \"Year = 2020\")\n",
    "# df19 = get_fs_data_query(devhistoryURL, \"Year = 2019\")\n",
    "# df18 = get_fs_data_query(devhistoryURL, \"Year = 2018\")\n",
    "df12 = get_fs_data_query(devhistoryURL, \"Year = 2012\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web service url\n",
    "permitTable = \"https://maps.trpa.org/server/rest/services/Permit_Records/MapServer/1\"\n",
    "# get permit data as a dataframe\n",
    "dfPermit = get_fs_data(permitTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permitTable = os.path.join(sdeTabular, \"dbo.Accela_Record_Details\")\n",
    "# geodatabase table to pandas dataframe\n",
    "dfPermit = pd.DataFrame.spatial.from_table(dfPermit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer grid\n",
    "transferGrid = pd.read_csv(local_path/\"data\\raw_data\\TransactedAndBankedDevelopmentRights.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelaRecorDetails = \"https://laketahoeinfo.org/Api/GetAccelaRecordDetailsExcel/1A77D078-B83E-44E0-8CA5-8D7429E1A6B4\"\n",
    "# use requests to get the file downloaded to raw data folder\n",
    "# get_file(accelaRecorDetails, local_path/\"data/raw_data/Accela_Record_Details.xlsx\")\n",
    "with open(local_path/\"data/raw_data/Accela_Record_Details.xlsx\", 'rb') as f:\n",
    "    dfAccela = pd.read_excel(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change File_Date to datetime\n",
    "dfPermit['File_Date'] = pd.to_datetime(dfPermit['File_Date'], errors='coerce')\n",
    "# file datqe to date time\n",
    "dfPermit['File_Date'] = dfPermit['File_Date'].dt.date\n",
    "# filter by year\n",
    "dfPermit = dfPermit.loc[dfPermit['File_Date'].dt.year == 2024]\n",
    "# get unique values of Permit_Type\n",
    "dfPermit['Permit_Type'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADU/RBU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils import *\n",
    "\n",
    "sdeCollect = \"F:\\GIS\\DB_CONNECT\\Collection.sde\"\n",
    "sdeBase    = \"F:\\GIS\\DB_CONNECT\\Vector.sde\"\n",
    "# source data\n",
    "adu_table = sdeCollect + \"\\\\SDE.AccessoryDwellingUnit\"\n",
    "rbu_table = sdeCollect + \"\\\\SDE.ResidentialBonusUnit\"\n",
    "parcels   = sdeBase + \"\\\\sde.SDE.Parcels\\\\sde.SDE.Parcel_Master\"\n",
    "# data to dataframe\n",
    "dfADU = gdb_table_to_df(adu_table)\n",
    "dfRBU = gdb_table_to_df(rbu_table)\n",
    "sdfParcels = pd.DataFrame.spatial.from_featureclass(parcels)\n",
    "\n",
    "local_gdb = \n",
    "#### RBU Data ####\n",
    "# fields to keep\n",
    "rbu_fields = [\n",
    "        'APN',\n",
    "        'Transaction_ID',\n",
    "        'Created_Year',\n",
    "        'File_Number',\n",
    "        'Quantity',\n",
    "        'Incentive_Type',\n",
    "        'Allocation_Pool_Type',\n",
    "        'Reporting_category',\n",
    "        'Status',\n",
    "        'Status_Detail',\n",
    "        'Allocation_Pool',\n",
    "        'Receiving',\n",
    "        'Receiving_Jurisdiction',\n",
    "        'Receiving_Parcel_Nickname',\n",
    "        'Comments',\n",
    "        'Issued_Date',\n",
    "        'Pre_Grade_Date',\n",
    "        'Finalized_Date',\n",
    "        'APO_ADDRESS',\n",
    "        'JURISDICTION',\n",
    "        'PSTL_STATE',\n",
    "        'EXISTING_LANDUSE',\n",
    "        'LOCATION_TO_TOWNCENTER',\n",
    "        'WITHIN_BONUSUNIT_BNDY',\n",
    "        'WITHIN_TRPA_BNDY',\n",
    "        'PLAN_ID',\n",
    "        'PLAN_NAME',\n",
    "        'ZONING_ID',\n",
    "        'ZONING_DESCRIPTION',\n",
    "        'LOCAL_PLAN_HYPERLINK',\n",
    "        'DESIGN_GUIDELINES_HYPERLINK',\n",
    "        'LTINFO_HYPERLINK',\n",
    "        'PARCEL_ACRES',\n",
    "        'PARCEL_SQFT',\n",
    "        'SHAPE']\n",
    "\n",
    "# merge parcels to rbu\n",
    "sdfRBU = pd.merge(dfRBU, sdfParcels, how='left', left_on='Receiving', right_on='APN')\n",
    "sdfRBU = sdfRBU[rbu_fields]\n",
    "# output name\n",
    "outfc = os.path.join(local_gdb, \"Parcel_RBU_Staging\")\n",
    "# Export to geodatabase\n",
    "sdfRBU.spatial.to_featureclass(outfc, sanitize_columns=False, overwrite=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update ADU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate/append base table\n",
    "arcpy.management.TruncateTable(\n",
    "    in_table=r\"F:\\GIS\\DB_CONNECT\\Collection.sde\\SDE.AccessoryDwellingUnit\"\n",
    ")\n",
    "\n",
    "arcpy.management.Append(\n",
    "    inputs=r\"H:\\DataTransfer\\Parcel_Reporting_ADU_072325.csv\",\n",
    "    target=r\"F:\\GIS\\DB_CONNECT\\Collection.sde\\SDE.AccessoryDwellingUnit\",\n",
    "    schema_type=\"NO_TEST\",\n",
    "    field_mapping=r'File_Number \"File Number\" true true false 8000 Text 0 0,First,#,H:\\DataTransfer\\Parcel_Reporting_ADU_072325.csv,File_Number,0,7999;Project_Status \"Project Status\" true true false 8000 Text 0 0,First,#,H:\\DataTransfer\\Parcel_Reporting_ADU_072325.csv,Project_Status,0,7999;Parcel_Number \"Parcel Number\" true true false 8000 Text 0 0,First,#,H:\\DataTransfer\\Parcel_Reporting_ADU_072325.csv,Parcel_Number,0,7999;Jurisdiction \"Jurisdiction\" true true false 8000 Text 0 0,First,#,H:\\DataTransfer\\Parcel_Reporting_ADU_072325.csv,Jurisdiction,0,7999;Project_Notes \"Project Notes\" true true false 1073741822 Text 0 0,First,#,H:\\DataTransfer\\Parcel_Reporting_ADU_072325.csv,Project_Notes,0,7999;Units \"Number of Unit(s)\" true true false 8 Double 8 38,First,#,H:\\DataTransfer\\Parcel_Reporting_ADU_072325.csv,Units,-1,-1;RBU \"RBU\" true true false 8000 Text 0 0,First,#,H:\\DataTransfer\\Parcel_Reporting_ADU_072325.csv,RBU,0,7999;Source_of_Unit \"Source of Unit\" true true false 8000 Text 0 0,First,#,H:\\DataTransfer\\Parcel_Reporting_ADU_072325.csv,Source_of_Unit,0,7999;RBU_Type \"RBU Type\" true true false 8000 Text 0 0,First,#,H:\\DataTransfer\\Parcel_Reporting_ADU_072325.csv,RBU_Type,0,7999;RBU_Type_Details \"RBU Type Details\" true true false 8000 Text 0 0,First,#,H:\\DataTransfer\\Parcel_Reporting_ADU_072325.csv,RBU_Type_Details,0,7999;DR_Document_Number \"DR Document Number\" true true false 8000 Text 0 0,First,#,H:\\DataTransfer\\Parcel_Reporting_ADU_072325.csv,DR_Document_Number,0,7999;DR_Description \"DR Description\" true true false 1073741822 Text 0 0,First,#,H:\\DataTransfer\\Parcel_Reporting_ADU_072325.csv,DR_Description,0,7999;ADU_SqFt \"Average Size of ADU (sqft)\" true true false 8 Double 8 38,First,#,H:\\DataTransfer\\Parcel_Reporting_ADU_072325.csv,ADU_SqFt,-1,-1;Constructed \"Constructed\" true true false 8000 Text 0 0,First,#,H:\\DataTransfer\\Parcel_Reporting_ADU_072325.csv,Constructed,0,7999;Open_Date \"Open Date\" true true false 8 Date 0 0,First,#,H:\\DataTransfer\\Parcel_Reporting_ADU_072325.csv,Open_Date,-1,-1;Complete_Date \"Complete Date\" true true false 8 Date 0 0,First,#,H:\\DataTransfer\\Parcel_Reporting_ADU_072325.csv,Complete_Date,-1,-1;Issued_Date \"Issued Date\" true true false 8 Date 0 0,First,#,H:\\DataTransfer\\Parcel_Reporting_ADU_072325.csv,Issued_Date,-1,-1;Acknowledged_Date \"Acknowledged Date\" true true false 8 Date 0 0,First,#,H:\\DataTransfer\\Parcel_Reporting_ADU_072325.csv,Acknowledged_Date,-1,-1;Finalized_Date \"Date Finalized\" true true false 8 Date 0 0,First,#,H:\\DataTransfer\\Parcel_Reporting_ADU_072325.csv,Finalized_Date,-1,-1;GlobalID \"GlobalID\" false false true 38 GlobalID 0 0,First,#;created_user \"created_user\" true true false 255 Text 0 0,First,#;created_date \"created_date\" true true false 8 Date 0 0,First,#;last_edited_user \"last_edited_user\" true true false 255 Text 0 0,First,#;last_edited_date \"last_edited_date\" true true false 8 Date 0 0,First,#',\n",
    "    subtype=\"\",\n",
    "    expression=\"\",\n",
    "    match_fields=None,\n",
    "    update_geometry=\"NOT_UPDATE_GEOMETRY\",\n",
    "    enforce_domains=\"NO_ENFORCE_DOMAINS\",\n",
    "    feature_service_mode=\"USE_FEATURE_SERVICE_MODE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "\"F:\\GIS\\DB_CONNECT\\Collection.sde\\SDE.AccessoryDwellingUnit\" does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m parcels   \u001b[38;5;241m=\u001b[39m sdeBase \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msde.SDE.Parcels\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msde.SDE.Parcel_Master\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# data to dataframe\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m dfADU \u001b[38;5;241m=\u001b[39m gdb_table_to_df(\u001b[38;5;28mstr\u001b[39m(adu_table))\n\u001b[0;32m     13\u001b[0m sdfParcels \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mspatial\u001b[38;5;241m.\u001b[39mfrom_featureclass(parcels)\n\u001b[0;32m     15\u001b[0m local_gdb \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDataTransfer\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mWorkspace.gdb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mbindl\\Documents\\GitHub\\Reporting\\utils.py:27\u001b[0m, in \u001b[0;36mgdb_table_to_df\u001b[1;34m(table_path)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGDB not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(table_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Exclude geometry fields\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m fields \u001b[38;5;241m=\u001b[39m [f\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m arcpy\u001b[38;5;241m.\u001b[39mListFields(table_path)]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Extract data\u001b[39;00m\n\u001b[0;32m     30\u001b[0m data \u001b[38;5;241m=\u001b[39m [row \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m arcpy\u001b[38;5;241m.\u001b[39mda\u001b[38;5;241m.\u001b[39mSearchCursor(table_path, fields)]\n",
      "File \u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\__init__.py:1228\u001b[0m, in \u001b[0;36mListFields\u001b[1;34m(dataset, wild_card, field_type)\u001b[0m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;129m@_gptooldoc\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, [[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatureLayer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTableView\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatureDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mListFields\u001b[39m(dataset, wild_card\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, field_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"ListFields(dataset, {wild_card}, {field_type})\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \n\u001b[0;32m   1189\u001b[0m \u001b[38;5;124;03m       Lists the fields in a feature class, shapefile, or table in a specified\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \n\u001b[0;32m   1227\u001b[0m \u001b[38;5;124;03m        * String:   Only field types of String are returned.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gp\u001b[38;5;241m.\u001b[39mlistFields(dataset, wild_card, field_type)\n",
      "File \u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\geoprocessing\\_base.py:378\u001b[0m, in \u001b[0;36mGeoprocessor.listFields\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"GP function ListFields\"\"\"\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01marcpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marcobjects\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marcobjectconversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convertArcObjectToPythonObject\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m convertArcObjectToPythonObject(\n\u001b[1;32m--> 378\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gp\u001b[38;5;241m.\u001b[39mListFields(\u001b[38;5;241m*\u001b[39mgp_fixargs(args, \u001b[38;5;28;01mTrue\u001b[39;00m)))\n",
      "\u001b[1;31mOSError\u001b[0m: \"F:\\GIS\\DB_CONNECT\\Collection.sde\\SDE.AccessoryDwellingUnit\" does not exist"
     ]
    }
   ],
   "source": [
    "# \n",
    "from utils import *\n",
    "from pathlib import Path\n",
    "\n",
    "sdeCollect = Path(\"F:\\GIS\\DB_CONNECT\\Collection.sde\")\n",
    "sdeBase    = Path(\"F:\\GIS\\DB_CONNECT\\Vector.sde\")\n",
    "\n",
    "adu_table = sdeCollect / \"SDE.AccessoryDwellingUnit\"\n",
    "parcels   = sdeBase / \"sde.SDE.Parcels\\\\sde.SDE.Parcel_Master\"\n",
    "\n",
    "# data to dataframe\n",
    "dfADU = gdb_table_to_df(str(adu_table))\n",
    "sdfParcels = pd.DataFrame.spatial.from_featureclass(parcels)\n",
    "\n",
    "local_gdb = \"H:\\DataTransfer\\Workspace.gdb\"\n",
    "\n",
    "#### ADU Data ####\n",
    "# adu export\n",
    "adu_fields = [\n",
    "        'APN',\n",
    "        'File_Number',\n",
    "        'Project_Status',\n",
    "        'Parcel_Number',\n",
    "        'Jurisdiction',\n",
    "        'Project_Notes',\n",
    "        'Units',\n",
    "        'RBU',\n",
    "        'Source_of_Unit',\n",
    "        'RBU_Type',\n",
    "        'RBU_Type_Details',\n",
    "        'DR_Document_Number',\n",
    "        'DR_Description',\n",
    "        'ADU_SqFt',\n",
    "        'Constructed',\n",
    "        'Open_Date',\n",
    "        'Complete_Date',\n",
    "        'Issued_Date',\n",
    "        'Acknowledged_Date',\n",
    "        'Finalized_Date',\n",
    "        'APO_ADDRESS',\n",
    "        'JURISDICTION',\n",
    "        'PSTL_STATE',\n",
    "        'EXISTING_LANDUSE',\n",
    "        'LOCATION_TO_TOWNCENTER',\n",
    "        'WITHIN_BONUSUNIT_BNDY',\n",
    "        'WITHIN_TRPA_BNDY',\n",
    "        'PLAN_ID',\n",
    "        'PLAN_NAME',\n",
    "        'ZONING_ID',\n",
    "        'ZONING_DESCRIPTION',\n",
    "        'LOCAL_PLAN_HYPERLINK',\n",
    "        'DESIGN_GUIDELINES_HYPERLINK',\n",
    "        'LTINFO_HYPERLINK',\n",
    "        'PARCEL_ACRES',\n",
    "        'PARCEL_SQFT',\n",
    "        'SHAPE']\n",
    "\n",
    "# merge parcels\n",
    "sdfADU = pd.merge(dfADU, sdfParcels, how='left', left_on='Parcel_Number', right_on='APN')\n",
    "sdfADU = sdfADU[adu_fields]\n",
    "# output name\n",
    "outfc = os.path.join(local_gdb, \"Parcel_ADU_Staging\")\n",
    "# Export to geodatabase\n",
    "sdfADU.spatial.to_featureclass(outfc, sanitize_columns=False, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development Rigth Transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permit Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRPA Permit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Data***\n",
    "> TRPA permit data is exported from accela nightly then stored in colleciton.sde enterprise geodatabase and published to the trpa server as the web service below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRPA Permit Data Engineering\n",
    "dfPermit.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Transformation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfPermit.copy()\n",
    "\n",
    "# final fields for all permit dataframes\n",
    "fields = ['APN', 'Address', 'Jurisdiction', 'Permit_ID', \n",
    "          'Permit_Type','Permit_Category', 'Permit_Status',  'Description',\n",
    "          'Applied_Date', 'Issued_Date', 'PreGrade_Date', 'Finaled_Date'\n",
    "          ]\n",
    "\n",
    "# # set fields\n",
    "column_mapping = {\n",
    "'Accela_ID' : 'Permit_ID',\n",
    "'Detailed_Description' : 'Description',\n",
    "'Record_Status' : 'Permit_Status',\n",
    "'Accela_CAPType_Name' : 'Permit_Type',\n",
    "'File_Date' : 'Applied_Date'\n",
    "}\n",
    "\n",
    "# rename columns based on dictionary\n",
    "df = renamecolumns(df, column_mapping, False)\n",
    "\n",
    "# add missing fields\n",
    "for field in fields:\n",
    "    # if field not in dataframe add it\n",
    "    if field not in df.columns:\n",
    "        # insert new column\n",
    "        df[field] = None\n",
    "# limit to the final fields\n",
    "df = df[fields]\n",
    "# add jurisdiction value\n",
    "df.Jurisdiction = \"TRPA\"\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Processing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out unique Record_Status values one at a time\n",
    "for description in df.Detailed_Description.unique():\n",
    "    print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out unique Record_Status values one at a time\n",
    "for permittype in df.Accela_CAPType_Name.unique():\n",
    "    print(permittype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out unique Record_Status values one at a time\n",
    "for status in df.Record_Status.unique():\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_lookup = \"resources\\Value_Lookups.csv\"\n",
    "trpa_reportingcategory_lookup = import_lookup_dictionary(value_lookup,'key','value','Jurisdiction','TRPA','FieldName','Reporting_Category')\n",
    "trpa_permittype_lookup        = import_lookup_dictionary(value_lookup,'key','value','Jurisdiction','TRPA','FieldName','Permit_Type')\n",
    "trpa_permitstatus_lookup      = import_lookup_dictionary(value_lookup,'key','value','Jurisdiction','TRPA','FieldName','Permit_Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update fields from lookup dictionaries\n",
    "df['Reporting_Category'] = df['Reporting_Category'].map(trpa_reportingcategory_lookup)\n",
    "df['Permit_Type'] = df['Permit_Type'].map(trpa_permittype_lookup)\n",
    "df['Permit_Status'] = df['Permit_Status'].map(trpa_permitstatus_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### City of South Lake Tahoe Permit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## City of South Lake Tahoe Permit data was sent over by Ryan Malhoski on 4/9/2021\n",
    "dfCSLTPermit = read_file(\"data\\PermitData_CSLT_040924.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCSLTPermit.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Transformation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop existing 'Address' field\n",
    "df = dfCSLTPermit.drop('Address', axis=1)\n",
    "\n",
    "# final fields for all permit dataframes\n",
    "fields = ['APN', 'Address', 'Jurisdiction', \n",
    "          'Permit_ID', 'Permit_Type','Permit_Status', 'Description',\n",
    "          'Applied_Date', 'Issued_Date', 'Finaled_Date'\n",
    "          ]\n",
    "\n",
    "# # set fields\n",
    "column_mapping = {\n",
    "            'Parcel ID': 'APN',\n",
    "            'Location Address':'Address',\n",
    "            'Permit Number' : 'Permit_ID',\n",
    "            'Note Text' : 'Description',\n",
    "            'Status' : 'Permit_Status',\n",
    "            'Permit Type' : 'Permit_Type',\n",
    "            'Permit Issue Date' : 'Applied_Date',\n",
    "            'Certificate Issue Date': \"Finaled_Date\"\n",
    "            }\n",
    "\n",
    "# rename columns based on dictionary\n",
    "df = renamecolumns(df, column_mapping,False)\n",
    "\n",
    "# add missing fields\n",
    "for field in fields:\n",
    "    # if field not in dataframe add it\n",
    "    if field not in df.columns:\n",
    "        # insert new column\n",
    "        df[field] = None\n",
    "# limit to the final fields\n",
    "df = df[fields]\n",
    "# add jurisdiction value\n",
    "df.Jurisdiction = \"CSLT\"\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APN is a PPNO format in the CSLT data, and also contains EL old naming convetion (-0)\n",
    "# need to format to xxx-xxx-xxx and filter any odd values (e.g. 500 series)\n",
    "# get rid of 100's and 500's series, and format to xxx-xxx-xxx, also remove any that start with strings\n",
    "# strip off trailing spaces\n",
    "df.APN = df.APN.str.replace(' ', '') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Processing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential values for Permit Type\n",
    "# \n",
    "# get unique permit types\n",
    "for permittype in dfCSLTPermit[\"Permit Type\"].unique():\n",
    "    print(permittype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El Dorado County Permit Data\n",
    ">  there are two files, one for all TRPA files and one for all files in our geographic area, including TRPA files and EDC files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## El Dorado Permit data representing all files in our geographic area\n",
    "## exported by Ken Kasman on 4/1/2021 from their Trakit database\n",
    "dfElDoPermit = read_file(\"data\\PermitData_ElDorado_040124.csv\")\n",
    "dfElDoPermit.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Transformation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop existing 'Address' field\n",
    "df = dfElDoPermit\n",
    "\n",
    "# final fields for all permit dataframes\n",
    "fields = ['APN', 'Address', 'Jurisdiction', \n",
    "          'Permit_ID', 'Permit_Type','Permit_Status','Description',\n",
    "          'Applied_Date', 'Issued_Date', 'Finaled_Date'\n",
    "          ]\n",
    "\n",
    "# # set fields\n",
    "column_mapping = {\n",
    "            'SITE_APN' : 'APN',\n",
    "            'SITE_ADDR':'Address',\n",
    "            'Permit Number' : 'Permit_ID',\n",
    "            'DESCRIPTION' : 'Description',\n",
    "            'STATUS' : 'Permit_Status',\n",
    "            'PERMITTYPE' : 'Permit_Type',\n",
    "            'APPLIED' : 'Applied_Date',\n",
    "            'ISSUED'  : 'Issued_Date',\n",
    "            'FINALED' : \"Finaled_Date\"\n",
    "            }\n",
    "\n",
    "# rename columns based on dictionary\n",
    "df = renamecolumns(df, column_mapping, False)\n",
    "\n",
    "# add missing fields\n",
    "for field in fields:\n",
    "    # if field not in dataframe add it\n",
    "    if field not in df.columns:\n",
    "        # insert new column\n",
    "        df[field] = None\n",
    "# limit to the final fields\n",
    "df = df[fields]\n",
    "# add jurisdiction value\n",
    "df.Jurisdiction = \"EL\"\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for permittype in dfElDoPermit[\"PERMITTYPE\"].unique():\n",
    "    print(permittype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lookup dictionary\n",
    "lookupTable = read_file(\"resources/lookup_reporting_category.csv\")\n",
    "lookupTable[\"Reporting Category\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Processing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Placer County Permit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Placer Permit Data Comes in monthly via email, and gets saved to the folder below.\n",
    "## The code below will merge all the files in the folder into a single file, return a dataframe, and export to csv\n",
    "\n",
    "# folder with the CSV files\n",
    "folder_path = r\"F:\\Research and Analysis\\Local Jurisdiction MOU data collection\\Placer MOU Files\\Placer\"\n",
    "# List to hold the DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through the files in the folder and identify CSV files\n",
    "for file_name in os.listdir(folder_path):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    # Read the CSV file into a DataFrame and append to the list\n",
    "    df = pd.read_excel(file_path)\n",
    "    # Append the DataFrame to the list\n",
    "    dfs.append(df)\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "# Add today's date at the end of the file name _MMDDYY\n",
    "today = pd.Timestamp.today().strftime(\"%m%d%y\")\n",
    "# Export the final DataFrame to a CSV file\n",
    "final_df.to_csv(\"data\\PermitData_Placer_\" + today + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Placer Permit data explained above. \n",
    "dfPlacerPermit =read_file(\"data\\PermitData_Placer_040924.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPlacerPermit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPlacerPermit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Transformation***\n",
    "> hyperlink to Placer Accela record can be bulit using SERV_PROD_CODE, B1_PER_ID1, B1_PER_ID2, B1_PER_ID3\n",
    "* https://permits.placer.ca.gov/CitizenAccess/Cap/CapDetail.aspx?Module=TRPA&TabName=TRPA&capID1=16CAP&capID2=00000&capID3=0036O&agencyCode=PLACERCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lookup dictionary\n",
    "lookupTable = read_file(\"resources/PL_lookup_reporting_category.csv\")\n",
    "lookupTable[\"Reporting Category\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Processing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merege the processed dfs\n",
    "df = pd.concat([dfTRPA, dfCSLT, dfEL, dfPL], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data\\PermitData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Accounting Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VHR Unit Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfIPES.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2012, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "# get parcel history data\n",
    "parcel_history = os.path.join(sdeEdit, \"SDE.Parcel\\\\SDE.Parcel_History_Attributed\")\n",
    "sdfParcelStack = pd.DataFrame.spatial.from_featureclass(parcel_history)\n",
    "# filter to years in years list\n",
    "sdfParcelStack = sdfParcelStack.loc[sdfParcelStack['YEAR'].isin(years)]\n",
    "\n",
    "# get vhr data\n",
    "vhr = os.path.join(sdeCollect, \"SDE.Parcel\\\\SDE.Parcel_VHR\")\n",
    "sdfVHR = pd.DataFrame.spatial.from_featureclass(vhr)\n",
    "# filter sdfVHR to Status = Active\n",
    "sdfVHR = sdfVHR.loc[sdfVHR['Status'] == 'Active']\n",
    "\n",
    "# get ipes data\n",
    "ipes = os.path.join(sdeCollect, \"SDE.Parcel\\\\SDE.Parcel_LTinfo_IPES\")\n",
    "sdfIPES = pd.DataFrame.spatial.from_featureclass(ipes)\n",
    "# filter IPES to IPESScorType == 'Official'\n",
    "sdfIPES = sdfIPES.loc[sdfIPES['IPESScoreType'] == 'Official']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IPES Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfParcel22 = sdfParcelStack.loc[sdfParcelStack['YEAR'] == 2022]\n",
    "# merge parcel history and IPES parcels\n",
    "sdfParcelHistory_IPES = pd.merge(sdfParcel22, sdfIPES, on='APN', how='left', indicator=True)\n",
    "# merge parcel history and VHR parcels\n",
    "sdfParcelHistory_VHR  = pd.merge(sdfParcel22, sdfVHR, on='APN', how='left', indicator=True)\n",
    "\n",
    "# get list of APNs that are in both VHR and parcel history and Residential_Units == 0\n",
    "dfFix_Res = sdfParcelHistory_VHR.loc[(sdfParcelHistory_VHR['_merge'] == 'both') & (sdfParcelHistory_VHR['Residential_Units'] == 0)]\n",
    "\n",
    "# get list of APNs that are in both IPES and parcel history and IPES_Score == 0\n",
    "dfFix_IPES = sdfParcelHistory_IPES.loc[(sdfParcelHistory_IPES['_merge'] == 'both') & (sdfParcelHistory_IPES['IPESScore'] == 0)& (sdfParcelHistory_IPES['Residential_Units'] > 0)].APN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apns = dfFix_Res.APN.to_list()\n",
    "# drop dupes from list\n",
    "apns = list(dict.fromkeys(apns))\n",
    "print(apns)\n",
    "for apn in apns:\n",
    "    print(apn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Existing Development Rights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attribution of Enterprise GDB Feature Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Spatial Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPATIAL JOINS ### THIS TAKES A LONG TIME ~3 HOURS ###\n",
    "\n",
    "# spatial join 2023 parcel data to all years of parcel data\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfParcel23, \"Join_23\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"WITHIN\")\n",
    "# spatial join to get Plan Area\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfPlan, \"Join_PlanArea\", \n",
    "                           \"JOIN_ONE_TO_MANY\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get District\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfDistrict, \"Join_District\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get Town Center\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfTownCenter, \"Join_TownCenter\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get Town Center Buffer\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfTCbuffer, \"Join_TownCenterBuffer\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get CSLT\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfCSLT, \"Join_CSLT\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get County\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfCounty, \"Join_County\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get TRPA Boundary\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfTRPA, \"Join_TRPA\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To Pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get results of spatial joins as spatial dataframes\n",
    "sdf_parcel_plan       = pd.DataFrame.spatial.from_featureclass(\"Join_PlanArea\", sr=sr)\n",
    "sdf_parcel_district   = pd.DataFrame.spatial.from_featureclass(\"Join_District\", sr=sr)\n",
    "sde_parcel_towncenter = pd.DataFrame.spatial.from_featureclass(\"Join_TownCenter\", sr=sr)\n",
    "sde_parcel_tcbuffer   = pd.DataFrame.spatial.from_featureclass(\"Join_TownCenterBuffer\", sr=sr)\n",
    "sde_parcel_cslt       = pd.DataFrame.spatial.from_featureclass(\"Join_CSLT\", sr=sr)\n",
    "sde_parcel_county     = pd.DataFrame.spatial.from_featureclass(\"Join_County\", sr=sr)\n",
    "sde_parcel_trpa       = pd.DataFrame.spatial.from_featureclass(\"Join_TRPA\", sr=sr)\n",
    "sde_parcel_23         = pd.DataFrame.spatial.from_featureclass(\"Join_23\", sr=sr)\n",
    "\n",
    "# pickling the dataframes\n",
    "sdf_parcel_plan.to_pickle(data_dir / \"sdf_parcel_plan.pkl\")\n",
    "sdf_parcel_district.to_pickle(data_dir / \"sdf_parcel_district.pkl\")\n",
    "sde_parcel_towncenter.to_pickle(data_dir / \"sde_parcel_towncenter.pkl\")\n",
    "sde_parcel_tcbuffer.to_pickle(data_dir / \"sde_parcel_tcbuffer.pkl\")\n",
    "sde_parcel_cslt.to_pickle(data_dir / \"sde_parcel_cslt.pkl\")\n",
    "sde_parcel_county.to_pickle(data_dir /\"sde_parcel_county.pkl\")\n",
    "sde_parcel_trpa.to_pickle(data_dir / \"sde_parcel_trpa.pkl\")\n",
    "sde_parcel_23.to_pickle(data_dir / \"sde_parcel_23.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Map Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pickled dataframes\n",
    "# get results of spatial joins as spatial dataframes\n",
    "sdf_parcel_plan       = pd.read_pickle(data_dir/\"sdf_parcel_plan.pkl\")\n",
    "sdf_parcel_district   = pd.read_pickle(data_dir/\"sdf_parcel_district.pkl\")\n",
    "sde_parcel_towncenter = pd.read_pickle(data_dir/\"sde_parcel_towncenter.pkl\")\n",
    "sde_parcel_tcbuffer   = pd.read_pickle(data_dir/\"sde_parcel_tcbuffer.pkl\")\n",
    "sde_parcel_cslt       = pd.read_pickle(data_dir/\"sde_parcel_cslt.pkl\")\n",
    "sde_parcel_county     = pd.read_pickle(data_dir/\"sde_parcel_county.pkl\")\n",
    "sde_parcel_trpa       = pd.read_pickle(data_dir/\"sde_parcel_trpa.pkl\")\n",
    "sde_parcel_23         = pd.read_pickle(data_dir/\"sde_parcel_23.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create uniqie key for spatial join APN _ YEAR\n",
    "\n",
    "for df in dfs:\n",
    "    df['APN_YEAR'] = df['APN'] + \"_\" + df['YEAR']\n",
    "    \n",
    "sdf_parcel_plan['APN_YEAR'] = sdf_parcel_plan['APN'] + \"_\" + sdf_parcel_plan['YEAR']\n",
    "sdf_parcel_district['APN_YEAR'] = sdf_parcel_district['APN'] + \"_\" + sdf_parcel_district['YEAR']\n",
    "sde_parcel_towncenter['APN_YEAR'] = sde_parcel_towncenter['APN'] + \"_\" + sde_parcel_towncenter['YEAR']\n",
    "sde_parcel_tcbuffer['APN_YEAR'] = sde_parcel_tcbuffer['APN'] + \"_\" + sde_parcel_tcbuffer['YEAR']\n",
    "sde_parcel_cslt['APN_YEAR'] = sde_parcel_cslt['APN'] + \"_\" + sde_parcel_cslt['YEAR']\n",
    "sde_parcel_county['APN_YEAR'] = sde_parcel_county['APN'] + \"_\" + sde_parcel_county['YEAR']\n",
    "sde_parcel_trpa['APN_YEAR'] = sde_parcel_trpa['APN'] + \"_\" + sde_parcel_trpa['YEAR']\n",
    "sde_parcel_23['APN_YEAR'] = sde_parcel_23['APN'] + \"_\" + sde_parcel_23['YEAR']\n",
    "sdfParcel['APN_YEAR'] = sdfParcel['APN'] + \"_\" + sdfParcel['YEAR']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the dataframes and get a list of columns for each\n",
    "for df in [sdf_parcel_plan, sdf_parcel_district, sde_parcel_towncenter, \n",
    "           sde_parcel_tcbuffer, sde_parcel_cslt, sde_parcel_county, \n",
    "           sde_parcel_trpa, sde_parcel_23]:\n",
    "    print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set up the sdfParcel dataframe with the 2023 parcel data values and then map the values from the other spatial join dataframes\n",
    "sdfParcel['JURISDICTION']      = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.JURISDICTION_1)))\n",
    "sdfParcel['COUNTY']            = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.COUNTY_1)))\n",
    "sdfParcel['COUNTY_LANDUSE_DESCRIPTION'] = sdfParcel.APN.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.COUNTY_LANDUSE_DESCRIPTION_1)))\n",
    "sdfParcel['EXISTING_LANDUSE']  = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.EXISTING_LANDUSE_1)))\n",
    "sdfParcel['OWNERSHIP_TYPE']    = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.OWNERSHIP_TYPE_1)))\n",
    "sdfParcel['YEAR_BUILT']        = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.YEAR_BUILT_1)))\n",
    "sdfParcel['PLAN_ID']           = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.PLAN_ID_1)))\n",
    "sdfParcel['PLAN_NAME']         = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.PLAN_NAME_1)))\n",
    "# sdfParcel['PLAN_TYPE']         = sdfParcel.APN.map(dict(zip(sde_parcel_23.APN, sde_parcel_23.PLAN_TYPE_1)))\n",
    "sdfParcel['ZONING_ID']         = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.ZONING_ID_1)))\n",
    "sdfParcel['ZONING_DESCRIPTION']= sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.ZONING_DESCRIPTION_1)))\n",
    "sdfParcel['TOWN_CENTER']       = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.TOWN_CENTER_1)))\n",
    "sdfParcel['LOCATION_TO_TOWNCENTER'] = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.LOCATION_TO_TOWNCENTER_1)))\n",
    "sdfParcel['TAZ']               = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.TAZ_1)))\n",
    "sdfParcel['PARCEL_ACRES']      = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.PARCEL_ACRES_1)))\n",
    "sdfParcel['PARCEL_SQFT']       = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.PARCEL_SQFT_1)))\n",
    "sdfParcel['WITHIN_BONUSUNIT_BNDY'] = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.WITHIN_BONUSUNIT_BNDY_1)))\n",
    "sdfParcel['WITHIN_TRPA_BNDY'] = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_23.APN_YEAR, sde_parcel_23.WITHIN_TRPA_BNDY_1)))\n",
    "\n",
    "# map dictionary to sdf_units dataframe to fill in TAZ and Block Group fields\n",
    "sdfParcel['PLAN_ID']     = sdfParcel.APN_YEAR.map(dict(zip(sdf_parcel_plan.APN_YEAR, sdf_parcel_plan.PLAN_ID_1)))\n",
    "sdfParcel['PLAN_NAME']   = sdfParcel.APN_YEAR.map(dict(zip(sdf_parcel_plan.APN_YEAR, sdf_parcel_plan.PLAN_NAME_1)))\n",
    "# sdfParcel['PLAN_TYPE']   = sdfParcel.APN.map(dict(zip(sdf_parcel_plan.APN, sdf_parcel_plan.PLAN_TYPE_1)))\n",
    "\n",
    "sdfParcel['ZONING_ID']     = sdfParcel.APN_YEAR.map(dict(zip(sdf_parcel_district.APN_YEAR, sdf_parcel_district.ZONING_ID_1)))\n",
    "sdfParcel['ZONING_DESCRIPTION']   = sdfParcel.APN_YEAR.map(dict(zip(sdf_parcel_district.APN_YEAR, sdf_parcel_district.ZONING_DESCRIPTION_1))\n",
    "                                                      ) \n",
    "# using sdf_parcel_towncenter and sdf_parcel_tcbuffer\n",
    "sdfParcel['TOWN_CENTER']              = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_towncenter.APN_YEAR, sde_parcel_towncenter.Name)))\n",
    "sdfParcel['LOCATION_TO_TOWNCENTER']   = sdfParcel.APN_YEAR.map(dict(zip(sde_parcel_tcbuffer.APN_YEAR, sde_parcel_tcbuffer.BUFFER_NAME)))\n",
    "# # using sdf_parcel_cslt and sdf_parcel_county\n",
    "# sdfParcel['JURISDICTION'] = sdfParcel.APN.map(dict(zip(sdf_parcel_cslt.APN, sdf_parcel_cslt.JURISDICTION)))\n",
    "# sdfParcel['COUNTY']       = sdfParcel.APN.map(dict(zip(sdf_parcel_county.APN, sdf_parcel_county.COUNTY)))\n",
    "\n",
    "# sdfParcel['WITHIN_BONUSUNIT_BNDY'] = sdfParcel.APN.map(dict(zip(sdf_parcel_trpa.APN, sdf_parcel_trpa.WITHIN_BONUS_UNIT_BNDY)))\n",
    "# sdfParcel['WITHIN_TRPA_BNDRY'] = sdfParcel.APN.map(dict(zip(sde_parcel_trpa.APN, sde_parcel_trpa.WITHIN_TRPA_BNDY_TRPA)))\n",
    "# sdfParcel to pickle\n",
    "sdfParcel.to_pickle(data_dir/\"sdfParcel.pkl\")\n",
    "# to feature class\n",
    "sdfParcel.spatial.to_featureclass(location=os.path.join(gdb,\"sdfParcel\"), sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Edit Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS TAKES 2 HOURS TO RUN ###\n",
    "\n",
    "# feature classes from sde\n",
    "sde_ParcelAtt    = sdeCollect + \"\\\\SDE.Parcel\\\\SDE.Parcel_History_Attributed\"\n",
    "# read in staging feature class\n",
    "sdfParcelFC = os.path.join(gdb, \"sdfParcel\")\n",
    "\n",
    "# start an edit session\n",
    "edit = arcpy.da.Editor(sdeCollect)  \n",
    "edit.startEditing(False, False)\n",
    "edit.startOperation()\n",
    "\n",
    "# use field join multikey to join the data\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['JURISDICTION'], \n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['JURISDICTION'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['COUNTY'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['COUNTY'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['COUNTY_LANDUSE_DESCRIPTION'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['COUNTY_LANDUSE_DESCRIPTION'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['EXISTING_LANDUSE'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['EXISTING_LANDUSE'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['OWNERSHIP_TYPE'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['OWNERSHIP_TYPE'])    \n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['YEAR_BUILT'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['YEAR_BUILT'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['PLAN_ID'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['PLAN_ID'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['PLAN_NAME'],    \n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['PLAN_NAME'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['ZONING_ID'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['ZONING_ID'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['ZONING_DESCRIPTION'],   \n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['ZONING_DESCRIPTION'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['TOWN_CENTER'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['TOWN_CENTER'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['LOCATION_TO_TOWNCENTER'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['LOCATION_TO_TOWNCENTER'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['TAZ'], \n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['TAZ'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['PARCEL_ACRES'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['PARCEL_ACRES'])\n",
    "fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['PARCEL_SQFT'],\n",
    "                       sdfParcelFC,   ['APN', 'YEAR'],['PARCEL_SQFT'])\n",
    "# fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['WITHIN_BONUSUNIT_BNDY'],\n",
    "#                        sdfParcelFC,   ['APN', 'YEAR'],['WITHIN_BONUSUNIT_BNDY'])\n",
    "# fieldJoinCalc_multikey(sde_ParcelAtt, ['APN', 'YEAR'],['WITHIN_TRPA_BNDY'], \n",
    "#                        sdfParcelFC,   ['APN', 'YEAR'],['WITHIN_TRPA_BNDY'])\n",
    "\n",
    "# close the edit session\n",
    "edit.stopOperation()\n",
    "edit.stopEditing(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature classes from sde\n",
    "sde_ParcelAtt    = sdeCollect+\"\\\\SDE.Parcel\\\\SDE.Parcel_History_Attributed\"\n",
    "# get data frame from feature class\n",
    "sdfParcel_SDE = pd.DataFrame.spatial.from_featureclass(sde_ParcelAtt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total Residenatial units by year\n",
    "residential_units = sdfParcel_SDE.groupby('YEAR')['Residential_Units'].sum()\n",
    "residential_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get change in residential units by year and jurisdiction\n",
    "residential_units = sdfParcel_SDE.groupby(['YEAR','ZONING_ID'])['Residential_Units'].sum()\n",
    "residential_units  = residential_units.unstack()\n",
    "# get difference between years\n",
    "differences = residential_units.diff(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences.to_csv(data_dir / \"Unit_Differences_by_Year_Zone_{today}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by year and sum residential units where years are in the list\n",
    "# filter to years\n",
    "sdfParcel_SDE = sdfParcel_SDE[sdfParcel_SDE.YEAR.isin(years)]\n",
    "residential_units = sdfParcel_SDE.groupby(['COUNTY','YEAR'])['Residential_Units'].sum()\n",
    "residential_units = residential_units.unstack()\n",
    "residential_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the change in development units year over year\n",
    "# group by year and county and sum residential units\n",
    "sdfParcelYear = sdfParcel_SDE.groupby(['COUNTY', 'YEAR']).agg({'Residential_Units':'sum'}).reset_index()\n",
    "# add difference column to get change in units year over year in each county\n",
    "sdfParcelYear['Difference'] = sdfParcelYear.groupby('COUNTY')['Residential_Units'].diff()\n",
    "sdfParcelYear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unitsTable = pd.read_csv(\"data\\qa_data\\CumulativeAccounting_2012to2023_Updated.csv\")\n",
    "# drop columns after 4th column\n",
    "unitsTable = unitsTable.iloc[:, :5]\n",
    "# remove commas from the columns\n",
    "unitsTable = unitsTable.replace(',','', regex=True)\n",
    "# convert Comm_Floor_Area to integer\n",
    "unitsTable['CommercialFloorArea_SqFt'] = unitsTable['CommercialFloorArea_SqFt'].astype(int)\n",
    "dfNew = sdfParcel_SDE\n",
    "dfOld = unitsTable\n",
    "# run the merge functions to export feature classes and get unjoined data as csv\n",
    "# merge_and_export(parcel_history, unitsTable, years)\n",
    "# check_duplicates(dfNew, dfOld, years)\n",
    "# get_unjoined(dfNew, dfOld, years)\n",
    "compare_totals(dfNew, dfOld, years)\n",
    "# find_different_rows(dfNew, dfOld, years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to years\n",
    "sdfParcel_SDE = sdfParcel_SDE[sdfParcel_SDE.YEAR.isin([2012,2018,2019,2020,2021,2022,2023])]\n",
    "# send to pickle\n",
    "sdfParcel_SDE.to_pickle(data_dir / \"sdfParcel_SDE.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export to CSV\n",
    "# df = parcel_history\n",
    "\n",
    "# columns to keep\n",
    "columns_to_keep = ['APN', 'Residential_Units', 'TouristAccommodation_Units',\n",
    "                    'CommercialFloorArea_SqFt', 'YEAR',\n",
    "                    'JURISDICTION', 'COUNTY', \n",
    "                    # 'ADU', 'RBU', 'Allocation','Deed_Restricted_Units',\n",
    "                    'OWNERSHIP_TYPE','EXISTING_LANDUSE',\n",
    "                    # 'WITHIN_TRPA_BNDY'\n",
    "                    'PARCEL_ACRES', 'PARCEL_SQFT']\n",
    "\n",
    "# add integer columns for RBU, ADU, Allocation, and Deed Restricted Units\n",
    "df['ADU'] = 0\n",
    "df['RBU'] = 0\n",
    "df['Allocation'] = 0\n",
    "df['Deed_Restricted_Units'] = 0\n",
    "\n",
    "# keep only the columns in the list\n",
    "df = df[columns_to_keep]\n",
    "# # filter to 2023\n",
    "# df = df[df.YEAR == 2023]\n",
    "\n",
    "# export to csv with date stamp in name\n",
    "today = pd.Timestamp.today().strftime(\"%m%d%y\")\n",
    "df.to_csv(\"data\\DevelopmentHistory_2023_\" + today + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deed Restrictions\n",
    "> Deed restricted unit research needs to be merged with LTinfo housing deed restricitons and parcel unit data from 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedUnits  = read_excel(\"data\\Housing_Deed_Restrcitions.xlsx\", 0)\n",
    "dfDeedLTinfo = pd.read_json(\"https://laketahoeinfo.org/WebServices/GetDeedRestrictedParcels/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedUnits.to_csv(\"data\\DeedRestricted_HousingUnits.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedUnits.Units.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedLTinfo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique values for deed restrcition type\n",
    "dfDeedLTinfo.DeedRestrictionType.unique()\n",
    "\n",
    "# filter to Affordable, Achievable, and Moderate\n",
    "dfDeedLTinfo = dfDeedLTinfo[dfDeedLTinfo.DeedRestrictionType.isin(['Affordable Housing', 'Moderate Income Housing', 'Achievable Housing'])]  \n",
    "\n",
    "# count of total records\n",
    "dfDeedLTinfo.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcelUnits22.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedUnitsMerge = dfDeedUnits.merge(dfDeedLTinfo, on='APN', how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedUnitsMerge._merge.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedLTinfo[dfDeedLTinfo.duplicated(subset=['APN','DeedRestrictionType'], keep=False)].sort_values('APN').to_csv(\"HousingDeedRestrictions_LTinfo_Duplicates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify duplicates unique by APN and \n",
    "dfDeedUnits[dfDeedUnits.duplicated(subset=['APN', 'Deed_Restriction_Type','Units'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify duplicates\n",
    "dfDeedUnitsMerge[dfDeedUnitsMerge.duplicated(subset=['APN'], keep=False)].sort_values(by='APN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedUnitsMerge.to_csv(\"HousingDeedRestrictions_All.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the deed restricted units with the parcel units\n",
    "dfDeedUnits_ParcelUnits  = dfDeedUnits.merge(parcelUnits22, on='APN', how='left')\n",
    "# merge the deed restricted units with the parcel units\n",
    "dfDeedLTinfo_ParcelUnits = dfDeedLTinfo.merge(parcelUnits22, left_on='APN', right_on='APN', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedLTinfo_ParcelUnits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedLTinfo_ParcelUnits.Residential_Units.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADU Tracking\n",
    "> ADU permit tracking from TRPA and othe Jurisdictions. There is a need to establish a system of record for this information (LT Info). This is similar to the Residential Bonus Unit data and theres crossover on some of these, where a bonus unit was used to create an ADU, but you can have an ADU without requiring a bonus unit, and you can use a bonus unit without it being an ADU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfADU = read_excel(\"data\\ADU Tracking.xlsx\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfADU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Allocations\n",
    "> This file includes all of the allocations that have been tracked in LT Info, and adds in whether the subject parcel has been issued a BMP/SCC certificate and/or whether Air Quality/Mobility Mitigation fees (for added VMT) or Water Quality Mitigation fees (for added coverage) have been paid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allocations = read_excel(\"data\\Allocation_Tracking.xlsx\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transactions with Inactive APNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inactiveParcels = read_file(\"data\\Transactions_InactiveParcels.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgispro-py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
