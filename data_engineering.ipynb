{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TO DO\n",
    "* Add ADU, Bonus Unit, and Allocation fields to Parcel_History_Attributed\n",
    "* Clean up unnecessary fields in Parcel_History_Attributed\n",
    "* Add VHR/Bedrooms to the data\n",
    "* Add CFA research from Ken to the data\n",
    "* Check totals year over year\n",
    "* Add in way to model TAUs as Residential units where City converted Hotels>Apartments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terminology "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data engineering can consist of ***collection, cleaning, transformation, processing, and automating and monitoring tasks***\n",
    "* Collection - examples include getting data from a rest service as a\n",
    "* Cleaning - categorizing \n",
    "* Transformation - cateogorizing, standardization, \n",
    "* Processing - algorithm, pivot, groupby, merge\n",
    "* Automating - schedule task, Apache Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Planning Jargon\n",
    "* ADU - Accessory Dwelling Unit\n",
    "* Existing Development Right - refers to residential, commercial, or tourist development currently built in the Lake Tahoe Basin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages, Maps, and Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from utils import *\n",
    "import getpass\n",
    "from arcgis.features import GeoAccessor, GeoSeriesAccessor\n",
    "from arcgis.mapping import show_styles, display_colormaps\n",
    "from arcgis.gis import GIS\n",
    "from utils import *\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Pandas Options***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data frame display options\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 2000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "arcpy.env.overwriteOutput = True    \n",
    "arcpy.env.workspace = 'memory'\n",
    "# # set workspace\n",
    "# arcpy.env.workspace = os.path.join(local_path, 'Workspace.gdb')\n",
    "# overwrite true\n",
    "arcpy.env.overwriteOutput = True\n",
    "# Set spatial reference to NAD 1983 UTM Zone 10N\n",
    "sr = arcpy.SpatialReference(26910)\n",
    "# # Set the extent environment using a feature class\n",
    "# arcpy.env.extent = \"Tahoe_OccupancyRate_Zones\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Map Setup***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the GIS object\n",
    "## portal URL = \"https://maps.trpa.org/portal/home/\"\n",
    "## AGOL URL   = \"https://www.arcgis.com\"\n",
    "gis = GIS(\n",
    "    url=\"https://maps.trpa.org/portal/home/\",\n",
    "    ## enter username above ##\n",
    "    username= input(\"Enter username:\"),\n",
    "    ## enter password above ##\n",
    "    password=getpass.getpass(\"Enter password:\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a map object\n",
    "map = gis.map(\"Lake Tahoe\", zoomlevel=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Reference Data***\n",
    "* https://www.laketahoeinfo.org/WebServices/List\n",
    "* https://maps.trpa.org/server/rest/services/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LT Info Data\n",
    "# Verified Development Rights\n",
    "dfDevRight  = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetParcelDevelopmentRightsForAccela/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "# Deed Restrictions as a DataFrame\n",
    "dfDeed      = pd.read_json(\"https://laketahoeinfo.org/WebServices/GetDeedRestrictedParcels/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "# IPES LTinfo as a DataFrame\n",
    "dfIPES      = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetParcelIPESScores/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "# Development Rights Transacted and Banked as a DataFrame\n",
    "dfDevRights = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetTransactedAndBankedDevelopmentRights/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "# All Parcels as a DataFrame\n",
    "dfLTParcel  = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetAllParcels/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRPA Data \n",
    "# Parcel Master as a Spatially Enabled Dataframe from a Feature Service\n",
    "sdfParcel     = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Parcels/FeatureServer/0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Could not load the dataset: 'dict' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\_impl\\common\\_query.py\u001b[0m in \u001b[0;36m_query_df\u001b[1;34m(layer, url, params, **kwargs)\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_con\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"features\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\gis\\_impl\\_con\\_connection.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, path, params, files, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1526\u001b[1;33m         return self._handle_response(\n\u001b[0m\u001b[0;32m   1527\u001b[0m             \u001b[0mresp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\gis\\_impl\\_con\\_connection.py\u001b[0m in \u001b[0;36m_handle_response\u001b[1;34m(self, resp, file_name, out_path, try_json, force_bytes, ignore_error_key)\u001b[0m\n\u001b[0;32m    982\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"error\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mignore_error_key\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 983\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"error\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    984\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: {'code': 500, 'message': 'Error performing query operation', 'details': []}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\features\\geo\\_accessor.py\u001b[0m in \u001b[0;36mfrom_layer\u001b[1;34m(layer)\u001b[0m\n\u001b[0;32m   3135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3136\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfrom_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3137\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\features\\geo\\_io\\serviceops.py\u001b[0m in \u001b[0;36mfrom_layer\u001b[1;34m(layer, query)\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid inputs: must be FeatureLayer or Table\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m     \u001b[0msdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m     \u001b[0msdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\features\\layer.py\u001b[0m in \u001b[0;36mquery\u001b[1;34m(self, where, out_fields, time_filter, geometry_filter, return_geometry, return_count_only, return_ids_only, return_distinct_values, return_extent_only, group_by_fields_for_statistics, statistic_filter, result_offset, result_record_count, object_ids, distance, units, max_allowable_offset, out_sr, geometry_precision, gdb_version, order_by_fields, out_statistics, return_z, return_m, multipatch_option, quantization_parameters, return_centroid, return_all_records, result_type, historic_moment, sql_format, return_true_curves, return_exceeded_limit_features, as_df, datum_transformation, time_reference_unknown_client, **kwargs)\u001b[0m\n\u001b[0;32m   2205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2206\u001b[1;33m         return _query._common_query(\n\u001b[0m\u001b[0;32m   2207\u001b[0m             \u001b[0mlayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\_impl\\common\\_query.py\u001b[0m in \u001b[0;36m_common_query\u001b[1;34m(layer, is_layer, where, text, out_fields, time_filter, geometry_filter, return_geometry, return_count_only, return_ids_only, return_distinct_values, return_extent_only, group_by_fields_for_statistics, statistic_filter, result_offset, result_record_count, object_ids, distance, units, max_allowable_offset, out_sr, geometry_precision, gdb_version, order_by_fields, out_statistics, return_z, return_m, multipatch_option, quantization_parameters, return_centroid, return_all_records, result_type, historic_moment, sql_format, return_true_curves, return_exceeded_limit_features, as_df, datum_transformation, range_values, parameter_values, format_3d_objects, time_reference_unknown_client, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mas_df\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_query_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\_impl\\common\\_query.py\u001b[0m in \u001b[0;36m_query_df\u001b[1;34m(layer, url, params, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m         ]\n\u001b[1;32m--> 537\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mqueryException\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"invalid token\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"token\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'lower'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28104\\1819113296.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## TRPA Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Parcel Master as a Spatially Enabled Dataframe from a Feature Service\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msdfParcel\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mget_fs_data_spatial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://maps.trpa.org/server/rest/services/Parcels/FeatureServer/0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# TRPA Boundary as a Spatially Enabled Dataframe from a Feature Service\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msdfBoundary\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mget_fs_data_spatial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://maps.trpa.org/server/rest/services/Boundaries/FeatureServer/4\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mbindl\\Documents\\GitHub\\Reporting\\utils.py\u001b[0m in \u001b[0;36mget_fs_data_spatial\u001b[1;34m(service_url)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_fs_data_spatial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mservice_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mfeature_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeatureLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mservice_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0msdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\features\\geo\\_accessor.py\u001b[0m in \u001b[0;36mfrom_layer\u001b[1;34m(layer)\u001b[0m\n\u001b[0;32m   3143\u001b[0m             )\n\u001b[0;32m   3144\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3145\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Could not load the dataset: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3147\u001b[0m     \u001b[1;31m# ----------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Could not load the dataset: 'dict' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "## TRPA Data \n",
    "# Parcel Master as a Spatially Enabled Dataframe from a Feature Service\n",
    "sdfParcel     = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Parcels/FeatureServer/0\")\n",
    "# TRPA Boundary as a Spatially Enabled Dataframe from a Feature Service\n",
    "sdfBoundary   = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/FeatureServer/4\")\n",
    "# Plan Area Boundary as a Spatially Enabled Dataframe from a Feature Service\n",
    "sdfPlanArea   = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/FeatureServer/0\")\n",
    "# District Boundary as a Spatially Enabled Dataframe from a Feature Service\n",
    "sdfDistrict   = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Zoning/FeatureServer/0\")\n",
    "# Town Center Boundary as a Spatially Enabled Dataframe from a Feature Service\n",
    "sdfTownCenter = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/FeatureServer/1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permit Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRPA Permit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Data***\n",
    "> TRPA permit data is exported from accela nightly then stored in colleciton.sde enterprise geodatabase and published to the trpa server as the web service below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web service url\n",
    "permitTable = \"https://maps.trpa.org/server/rest/services/Permit_Records/MapServer/1\"\n",
    "# get permit data as a dataframe\n",
    "dfTRPAPermit = get_fs_data(permitTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRPA Permit Data Engineering\n",
    "dfTRPAPermit.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Transformation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfTRPAPermit\n",
    "\n",
    "# final fields for all permit dataframes\n",
    "fields = ['APN', 'Address', 'Jurisdiction', 'Permit_ID', \n",
    "          'Permit_Type','Permit_Category', 'Permit_Status',  'Description',\n",
    "          'Applied_Date', 'Issued_Date', 'PreGrade_Date', 'Finaled_Date'\n",
    "          ]\n",
    "\n",
    "# # set fields\n",
    "column_mapping = {\n",
    "'Accela_ID' : 'Permit_ID',\n",
    "'Detailed_Description' : 'Description',\n",
    "'Record_Status' : 'Permit_Status',\n",
    "'Accela_CAPType_Name' : 'Permit_Type',\n",
    "'File_Date' : 'Applied_Date'\n",
    "}\n",
    "\n",
    "# rename columns based on dictionary\n",
    "df = renamecolumns(df, column_mapping, False)\n",
    "\n",
    "# add missing fields\n",
    "for field in fields:\n",
    "    # if field not in dataframe add it\n",
    "    if field not in df.columns:\n",
    "        # insert new column\n",
    "        df[field] = None\n",
    "# limit to the final fields\n",
    "df = df[fields]\n",
    "# add jurisdiction value\n",
    "df.Jurisdiction = \"TRPA\"\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Processing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out unique Record_Status values one at a time\n",
    "for description in dfTRPAPermit.Detailed_Description.unique():\n",
    "    print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out unique Record_Status values one at a time\n",
    "for permittype in dfTRPAPermit.Accela_CAPType_Name.unique():\n",
    "    print(permittype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out unique Record_Status values one at a time\n",
    "for status in dfTRPAPermit.Record_Status.unique():\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_lookup = \"resources\\Value_Lookups.csv\"\n",
    "trpa_reportingcategory_lookup = import_lookup_dictionary(value_lookup,'key','value','Jurisdiction','TRPA','FieldName','Reporting_Category')\n",
    "trpa_permittype_lookup        = import_lookup_dictionary(value_lookup,'key','value','Jurisdiction','TRPA','FieldName','Permit_Type')\n",
    "trpa_permitstatus_lookup      = import_lookup_dictionary(value_lookup,'key','value','Jurisdiction','TRPA','FieldName','Permit_Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update fields from lookup dictionaries\n",
    "df['Reporting_Category'] = df['Reporting_Category'].map(trpa_reportingcategory_lookup)\n",
    "df['Permit_Type'] = df['Permit_Type'].map(trpa_permittype_lookup)\n",
    "df['Permit_Status'] = df['Permit_Status'].map(trpa_permitstatus_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### City of South Lake Tahoe Permit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## City of South Lake Tahoe Permit data was sent over by Ryan Malhoski on 4/9/2021\n",
    "dfCSLTPermit = read_file(\"data\\PermitData_CSLT_040924.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCSLTPermit.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Transformation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop existing 'Address' field\n",
    "df = dfCSLTPermit.drop('Address', axis=1)\n",
    "\n",
    "# final fields for all permit dataframes\n",
    "fields = ['APN', 'Address', 'Jurisdiction', \n",
    "          'Permit_ID', 'Permit_Type','Permit_Status', 'Description',\n",
    "          'Applied_Date', 'Issued_Date', 'Finaled_Date'\n",
    "          ]\n",
    "\n",
    "# # set fields\n",
    "column_mapping = {\n",
    "            'Parcel ID': 'APN',\n",
    "            'Location Address':'Address',\n",
    "            'Permit Number' : 'Permit_ID',\n",
    "            'Note Text' : 'Description',\n",
    "            'Status' : 'Permit_Status',\n",
    "            'Permit Type' : 'Permit_Type',\n",
    "            'Permit Issue Date' : 'Applied_Date',\n",
    "            'Certificate Issue Date': \"Finaled_Date\"\n",
    "            }\n",
    "\n",
    "# rename columns based on dictionary\n",
    "df = renamecolumns(df, column_mapping,False)\n",
    "\n",
    "# add missing fields\n",
    "for field in fields:\n",
    "    # if field not in dataframe add it\n",
    "    if field not in df.columns:\n",
    "        # insert new column\n",
    "        df[field] = None\n",
    "# limit to the final fields\n",
    "df = df[fields]\n",
    "# add jurisdiction value\n",
    "df.Jurisdiction = \"CSLT\"\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APN is a PPNO format in the CSLT data, and also contains EL old naming convetion (-0)\n",
    "# need to format to xxx-xxx-xxx and filter any odd values (e.g. 500 series)\n",
    "# get rid of 100's and 500's series, and format to xxx-xxx-xxx, also remove any that start with strings\n",
    "# strip off trailing spaces\n",
    "df.APN = df.APN.str.replace(' ', '') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Processing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential values for Permit Type\n",
    "# \n",
    "# get unique permit types\n",
    "for permittype in dfCSLTPermit[\"Permit Type\"].unique():\n",
    "    print(permittype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El Dorado County Permit Data\n",
    ">  there are two files, one for all TRPA files and one for all files in our geographic area, including TRPA files and EDC files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## El Dorado Permit data representing all files in our geographic area\n",
    "## exported by Ken Kasman on 4/1/2021 from their Trakit database\n",
    "dfElDoPermit = read_file(\"data\\PermitData_ElDorado_040124.csv\")\n",
    "dfElDoPermit.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Transformation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop existing 'Address' field\n",
    "df = dfElDoPermit\n",
    "\n",
    "# final fields for all permit dataframes\n",
    "fields = ['APN', 'Address', 'Jurisdiction', \n",
    "          'Permit_ID', 'Permit_Type','Permit_Status','Description',\n",
    "          'Applied_Date', 'Issued_Date', 'Finaled_Date'\n",
    "          ]\n",
    "\n",
    "# # set fields\n",
    "column_mapping = {\n",
    "            'SITE_APN' : 'APN',\n",
    "            'SITE_ADDR':'Address',\n",
    "            'Permit Number' : 'Permit_ID',\n",
    "            'DESCRIPTION' : 'Description',\n",
    "            'STATUS' : 'Permit_Status',\n",
    "            'PERMITTYPE' : 'Permit_Type',\n",
    "            'APPLIED' : 'Applied_Date',\n",
    "            'ISSUED'  : 'Issued_Date',\n",
    "            'FINALED' : \"Finaled_Date\"\n",
    "            }\n",
    "\n",
    "# rename columns based on dictionary\n",
    "df = renamecolumns(df, column_mapping, False)\n",
    "\n",
    "# add missing fields\n",
    "for field in fields:\n",
    "    # if field not in dataframe add it\n",
    "    if field not in df.columns:\n",
    "        # insert new column\n",
    "        df[field] = None\n",
    "# limit to the final fields\n",
    "df = df[fields]\n",
    "# add jurisdiction value\n",
    "df.Jurisdiction = \"EL\"\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for permittype in dfElDoPermit[\"PERMITTYPE\"].unique():\n",
    "    print(permittype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lookup dictionary\n",
    "lookupTable = read_file(\"resources/lookup_reporting_category.csv\")\n",
    "lookupTable[\"Reporting Category\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Processing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Placer County Permit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Placer Permit Data Comes in monthly via email, and gets saved to the folder below.\n",
    "## The code below will merge all the files in the folder into a single file, return a dataframe, and export to csv\n",
    "\n",
    "# folder with the CSV files\n",
    "folder_path = r\"F:\\Research and Analysis\\Local Jurisdiction MOU data collection\\Placer MOU Files\\Placer\"\n",
    "# List to hold the DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through the files in the folder and identify CSV files\n",
    "for file_name in os.listdir(folder_path):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    # Read the CSV file into a DataFrame and append to the list\n",
    "    df = pd.read_excel(file_path)\n",
    "    # Append the DataFrame to the list\n",
    "    dfs.append(df)\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "# Add today's date at the end of the file name _MMDDYY\n",
    "today = pd.Timestamp.today().strftime(\"%m%d%y\")\n",
    "# Export the final DataFrame to a CSV file\n",
    "final_df.to_csv(\"data\\PermitData_Placer_\" + today + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Placer Permit data explained above. \n",
    "dfPlacerPermit =read_file(\"data\\PermitData_Placer_040924.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPlacerPermit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPlacerPermit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Transformation***\n",
    "> hyperlink to Placer Accela record can be bulit using SERV_PROD_CODE, B1_PER_ID1, B1_PER_ID2, B1_PER_ID3\n",
    "* https://permits.placer.ca.gov/CitizenAccess/Cap/CapDetail.aspx?Module=TRPA&TabName=TRPA&capID1=16CAP&capID2=00000&capID3=0036O&agencyCode=PLACERCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lookup dictionary\n",
    "lookupTable = read_file(\"resources/PL_lookup_reporting_category.csv\")\n",
    "lookupTable[\"Reporting Category\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Processing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merege the processed dfs\n",
    "df = pd.concat([dfTRPA, dfCSLT, dfEL, dfPL], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data\\PermitData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Accounting Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Existing Development Rights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get 2022 development units\n",
    "devhistoryURL = \"https://maps.trpa.org/server/rest/services/Existing_Development/MapServer/2\"\n",
    "# get parcel history for 2023\n",
    "df23 = get_fs_data_query(devhistoryURL, \"Year = 2023\")\n",
    "df22 = get_fs_data_query(devhistoryURL, \"Year = 2022\")\n",
    "df21 = get_fs_data_query(devhistoryURL, \"Year = 2021\")\n",
    "df20 = get_fs_data_query(devhistoryURL, \"Year = 2020\")\n",
    "df19 = get_fs_data_query(devhistoryURL, \"Year = 2019\")\n",
    "df18 = get_fs_data_query(devhistoryURL, \"Year = 2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Could not load the dataset: {'code': 500, 'message': 'Error handling service request :0x80004004 -  CreateDynamicMapService failed for service Boundaries.MapServer', 'details': []}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\gis\\__init__.py\u001b[0m in \u001b[0;36m_hydrate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  18742\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 18743\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_refresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  18744\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\gis\\__init__.py\u001b[0m in \u001b[0;36m_refresh\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  18716\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 18717\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  18718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\gis\\__init__.py\u001b[0m in \u001b[0;36m_refresh\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  18705\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 18706\u001b[1;33m                     dictdata = self._con.post(\n\u001b[0m\u001b[0;32m  18707\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\gis\\_impl\\_con\\_connection.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, path, params, files, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1526\u001b[1;33m         return self._handle_response(\n\u001b[0m\u001b[0;32m   1527\u001b[0m             \u001b[0mresp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\gis\\_impl\\_con\\_connection.py\u001b[0m in \u001b[0;36m_handle_response\u001b[1;34m(self, resp, file_name, out_path, try_json, force_bytes, ignore_error_key)\u001b[0m\n\u001b[0;32m    982\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"error\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mignore_error_key\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 983\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"error\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    984\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: {'code': 500, 'message': 'Error handling service request :0x80004004 -  CreateDynamicMapService failed for service Boundaries.MapServer', 'details': []}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\features\\geo\\_accessor.py\u001b[0m in \u001b[0;36mfrom_layer\u001b[1;34m(layer)\u001b[0m\n\u001b[0;32m   3135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3136\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfrom_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3137\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\features\\geo\\_io\\serviceops.py\u001b[0m in \u001b[0;36mfrom_layer\u001b[1;34m(layer, query)\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid inputs: must be FeatureLayer or Table\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m     \u001b[0msdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m     \u001b[0msdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\features\\layer.py\u001b[0m in \u001b[0;36mquery\u001b[1;34m(self, where, out_fields, time_filter, geometry_filter, return_geometry, return_count_only, return_ids_only, return_distinct_values, return_extent_only, group_by_fields_for_statistics, statistic_filter, result_offset, result_record_count, object_ids, distance, units, max_allowable_offset, out_sr, geometry_precision, gdb_version, order_by_fields, out_statistics, return_z, return_m, multipatch_option, quantization_parameters, return_centroid, return_all_records, result_type, historic_moment, sql_format, return_true_curves, return_exceeded_limit_features, as_df, datum_transformation, time_reference_unknown_client, **kwargs)\u001b[0m\n\u001b[0;32m   2205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2206\u001b[1;33m         return _query._common_query(\n\u001b[0m\u001b[0;32m   2207\u001b[0m             \u001b[0mlayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\_impl\\common\\_query.py\u001b[0m in \u001b[0;36m_common_query\u001b[1;34m(layer, is_layer, where, text, out_fields, time_filter, geometry_filter, return_geometry, return_count_only, return_ids_only, return_distinct_values, return_extent_only, group_by_fields_for_statistics, statistic_filter, result_offset, result_record_count, object_ids, distance, units, max_allowable_offset, out_sr, geometry_precision, gdb_version, order_by_fields, out_statistics, return_z, return_m, multipatch_option, quantization_parameters, return_centroid, return_all_records, result_type, historic_moment, sql_format, return_true_curves, return_exceeded_limit_features, as_df, datum_transformation, range_values, parameter_values, format_3d_objects, time_reference_unknown_client, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     params = _create_parameters(\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0mlayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\_impl\\common\\_query.py\u001b[0m in \u001b[0;36m_create_parameters\u001b[1;34m(layer, is_layer, where, text, out_fields, time_filter, geometry_filter, return_geometry, return_count_only, return_ids_only, return_distinct_values, return_extent_only, group_by_fields_for_statistics, statistic_filter, result_offset, result_record_count, object_ids, distance, units, max_allowable_offset, out_sr, geometry_precision, gdb_version, order_by_fields, out_statistics, return_z, return_m, multipatch_option, quantization_parameters, return_centroid, return_all_records, result_type, historic_moment, sql_format, return_true_curves, return_exceeded_limit_features, datum_transformation, range_values, parameter_values, format_3d_objects, time_reference_unknown_client, query_3d, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"returnM\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_is_3d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m             \u001b[1;31m# for 3D feature query\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\features\\layer.py\u001b[0m in \u001b[0;36m_is_3d\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"infoFor3D\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproperties\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproperties\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfoFor3D\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\gis\\__init__.py\u001b[0m in \u001b[0;36mproperties\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  18728\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 18729\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hydrate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  18730\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy_properties\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\gis\\__init__.py\u001b[0m in \u001b[0;36m_hydrate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  18765\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 18766\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_refresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  18767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\gis\\__init__.py\u001b[0m in \u001b[0;36m_refresh\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  18716\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 18717\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  18718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\gis\\__init__.py\u001b[0m in \u001b[0;36m_refresh\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  18705\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 18706\u001b[1;33m                     dictdata = self._con.post(\n\u001b[0m\u001b[0;32m  18707\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\gis\\_impl\\_con\\_connection.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, path, params, files, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1526\u001b[1;33m         return self._handle_response(\n\u001b[0m\u001b[0;32m   1527\u001b[0m             \u001b[0mresp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\gis\\_impl\\_con\\_connection.py\u001b[0m in \u001b[0;36m_handle_response\u001b[1;34m(self, resp, file_name, out_path, try_json, force_bytes, ignore_error_key)\u001b[0m\n\u001b[0;32m    982\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"error\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mignore_error_key\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 983\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"error\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    984\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: {'code': 500, 'message': 'Error handling service request :0x80004004 -  CreateDynamicMapService failed for service Boundaries.MapServer', 'details': []}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28104\\3058099112.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# sdfTownCenter = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/MapServer/1\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0msdfTCbuffer\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mget_fs_data_spatial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://maps.trpa.org/server/rest/services/Planning/MapServer/4\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0msdfCSLT\u001b[0m       \u001b[1;33m=\u001b[0m \u001b[0mget_fs_data_spatial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://maps.trpa.org/server/rest/services/Boundaries/MapServer/2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m# sdfCounty     = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/MapServer/3\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# sdfTRPA       = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/MapServer/4\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mbindl\\Documents\\GitHub\\Reporting\\utils.py\u001b[0m in \u001b[0;36mget_fs_data_spatial\u001b[1;34m(service_url)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_fs_data_spatial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mservice_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mfeature_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeatureLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mservice_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0msdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\arcgis\\features\\geo\\_accessor.py\u001b[0m in \u001b[0;36mfrom_layer\u001b[1;34m(layer)\u001b[0m\n\u001b[0;32m   3143\u001b[0m             )\n\u001b[0;32m   3144\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3145\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Could not load the dataset: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3147\u001b[0m     \u001b[1;31m# ----------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Could not load the dataset: {'code': 500, 'message': 'Error handling service request :0x80004004 -  CreateDynamicMapService failed for service Boundaries.MapServer', 'details': []}"
     ]
    }
   ],
   "source": [
    "## get parcel data\n",
    "# sdfParcel     = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Existing_Development/MapServer/2\")\n",
    "# get spatial data to join to\n",
    "sdfDistrict   = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Planning/MapServer/1\")\n",
    "# sdfPlan       = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/MapServer/0\")\n",
    "# sdfTownCenter = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/MapServer/1\")\n",
    "sdfTCbuffer   = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Planning/MapServer/4\")\n",
    "sdfCSLT       = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/MapServer/2\")\n",
    "# sdfCounty     = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/MapServer/3\")\n",
    "# sdfTRPA       = get_fs_data_spatial(\"https://maps.trpa.org/server/rest/services/Boundaries/MapServer/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sdfPlan' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28104\\2744727499.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# spatial join to get District\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m arcpy.SpatialJoin_analysis(sdfParcel, sdfPlan, \"Join_PlanArea\", \n\u001b[0m\u001b[0;32m      3\u001b[0m                            \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# spatial join to get Plan Area,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m arcpy.SpatialJoin_analysis(sdfParcel, sdfDistrict, \"Join_District\", \n",
      "\u001b[1;31mNameError\u001b[0m: name 'sdfPlan' is not defined"
     ]
    }
   ],
   "source": [
    "# spatial join to get District\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfPlan, \"Join_PlanArea\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get Plan Area, \n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfDistrict, \"Join_District\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get Town Center\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfTownCenter, \"Join_TownCenter\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get Town Center Buffer\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfTCbuffer, \"Join_TownCenterBuffer\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get CSLT\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfCSLT, \"Join_CSLT\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get County\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfCounty, \"Join_County\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get TRPA Boundary\n",
    "arcpy.SpatialJoin_analysis(sdfParcel, sdfTRPA, \"Join_TRPA\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get results of spatial joins as spatial dataframes\n",
    "sdf_parcel_plan = pd.DataFrame.spatial.from_featureclass(\"Join_PlanArea\", sr=sr)  \n",
    "sdf_parcel_zone = pd.DataFrame.spatial.from_featureclass(\"Join_Zone\", sr=sr)\n",
    "\n",
    "\n",
    "# map dictionary to sdf_units dataframe to fill in TAZ and Block Group fields\n",
    "sdfParcel['PLAN_AREA']     = sdfParcel.APN.map(dict(zip(sdf_parcel_plan.APN, sdf_parcel_plan.PLAN_NAME)))\n",
    "sdfParcel['BLOCK_GROUP']   = sdfParcel.APN.map(dict(zip(sdf_units_block.APN, sdf_units_block.TRPAID)))\n",
    "sdfParcel['OCCUPANCY_ZONE']= sdfParcel.APN.map(dict(zip(sdf_units_occ.APN,   sdf_units_occ.OccupancyRate_ZoneID)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spatial dataframe from spatial join feature class\n",
    "sdf_PlanJoin   = pd.DataFrame.spatial.from_featureclass(\"Join_PlanArea\", sr=sr)\n",
    "sdf_ZoneJoin   = pd.DataFrame.spatial.from_featureclass(\"Join_Zone\", sr=sr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map dictionary to sdf_units dataframe to fill in TAZ and Block Group fields\n",
    "df['PLAN_NAME_NEW'] = sdfParcel.APN.map(dict(zip(sdf_PlanJoin.APN, sdf_PlanJoin.NAME)))\n",
    "df['PLAN_ID_NEW']   = sdfParcel.APN.map(dict(zip(sdf_PlanJoin.APN, sdf_PlanJoin.ID)))\n",
    "df['ZONE_NAME']     = sdfParcel.APN.map(dict(zip(sdf_ZoneJoin.APN, sdf_ZoneJoin.NAME)))\n",
    "df['ZONE_ID']       = sdfParcel.APN.map(dict(zip(sdf_ZoneJoin.APN, sdf_ZoneJoin.ID)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_PlanJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the plan name and id\n",
    "df.groupby(['PLAN_ID', 'YEAR']).agg({'Residential_Units':'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot_table(index='APN', columns='YEAR', values=['Residential_Units','CommercialFloorArea_SqFt', 'TouristAccommodation_Units'], aggfunc='sum').reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get 2022 development units\n",
    "devhistoryURL = \"https://maps.trpa.org/server/rest/services/Existing_Development/MapServer/2\"\n",
    "# get parcel history for 2023\n",
    "df = get_fs_data_query(devhistoryURL, \"Year = 2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # groub by EXISTING_LANDUSE and aggregate Residential Units\n",
    "# df.groupby(\"EXISTING_LANDUSE\")[\"Residential_Units\"].sum()\n",
    "# flatten out as a dataframe\n",
    "df.groupby(\"EXISTING_LANDUSE\")[\"Residential_Units\"].sum().reset_index()\n",
    "# add a total field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# group by PLAN_NAME and sum Residential_Units\n",
    "df1 = df.groupby('PLAN_ID').agg({'Residential_Units':'sum', 'TouristAccommodation_Units':'sum','CommercialFloorArea_SqFt':'sum'}).reset_index()\n",
    "\n",
    "# print\n",
    "df1.sort_values('Residential_Units', ascending=False)\n",
    "\n",
    "# add total row\n",
    "df1.loc['Total'] = df1.sum(numeric_only=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export to CSV\n",
    "# df = parcel_history\n",
    "\n",
    "# columns to keep\n",
    "columns_to_keep = ['APN', 'Residential_Units', 'TouristAccommodation_Units',\n",
    "                    'CommercialFloorArea_SqFt', 'YEAR',\n",
    "                    'JURISDICTION', 'COUNTY', \n",
    "                    # 'ADU', 'RBU', 'Allocation','Deed_Restricted_Units',\n",
    "                    'OWNERSHIP_TYPE','EXISTING_LANDUSE',\n",
    "                    # 'WITHIN_TRPA_BNDY'\n",
    "                    'PARCEL_ACRES', 'PARCEL_SQFT']\n",
    "\n",
    "# add integer columns for RBU, ADU, Allocation, and Deed Restricted Units\n",
    "df['ADU'] = 0\n",
    "df['RBU'] = 0\n",
    "df['Allocation'] = 0\n",
    "df['Deed_Restricted_Units'] = 0\n",
    "\n",
    "# keep only the columns in the list\n",
    "df = df[columns_to_keep]\n",
    "# # filter to 2023\n",
    "# df = df[df.YEAR == 2023]\n",
    "\n",
    "# export to csv with date stamp in name\n",
    "today = pd.Timestamp.today().strftime(\"%m%d%y\")\n",
    "df.to_csv(\"data\\DevelopmentHistory_2023_\" + today + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get 2022 development units\n",
    "devhistoryURL = \"https://maps.trpa.org/server/rest/services/Existing_Development/MapServer/2\"\n",
    "parcel_history = get_fs_data_spatial(devhistoryURL)\n",
    "\n",
    "# get unit table as pandas dataframe\n",
    "unitsTable = pd.read_csv(\"data/CumulativeAccounting_2012to2023_Updated.csv\", low_memory=False)\n",
    "# get rid of columns after YEAR\n",
    "unitsTable.drop(unitsTable.columns[unitsTable.columns.get_loc(\"YEAR\")+1:], axis=1,inplace=True)\n",
    "# set cfa to numeric\n",
    "unitsTable['CommercialFloorArea_SqFt'] = pd.to_numeric(unitsTable['CommercialFloorArea_SqFt'], errors='coerce').fillna(0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "years = [2012, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "version = \"_v6_\"\n",
    "\n",
    "# merge parcel history and units table by year and \n",
    "# export to feature class\n",
    "def merge_and_export(parcel_history, unitsTable, years):\n",
    "    for year in years:\n",
    "        print(year)\n",
    "        # filter parcel_history by year\n",
    "        parcel_history_year = parcel_history.loc[parcel_history['YEAR'] == year]\n",
    "        # filter unitsTable by year\n",
    "        unitsTable_year = unitsTable.loc[unitsTable['YEAR'] == year]\n",
    "        # merge parcel_history_year and unitsTable_year\n",
    "        df = pd.merge(parcel_history_year, unitsTable_year, on='APN', how='left', indicator=True)\n",
    "        # make sure field types are numeric for Residential_Unit, TouristAccommodation_Units, and CommercialFloorArea_SqFt fields\n",
    "        df['Residential_Units']          = pd.to_numeric(df['Residential_Units_y'], errors='coerce')\n",
    "        df['TouristAccommodation_Units'] = pd.to_numeric(df['TouristAccommodation_Units_y'], errors='coerce')\n",
    "        df['CommercialFloorArea_SqFt']   = pd.to_numeric(df['CommercialFloorArea_SqFt_y'], errors='coerce')\n",
    "        # if NaN in Residential_Units, set to 0\n",
    "        df['Residential_Units'] = df['Residential_Units'].fillna(0)\n",
    "        # if NaN in TouristAccommodation_Units, set to 0\n",
    "        df['TouristAccommodation_Units'] = df['TouristAccommodation_Units'].fillna(0)\n",
    "        # if NaN in CommercialFloorArea_SqFt, set to 0\n",
    "        df['CommercialFloorArea_SqFt'] = df['CommercialFloorArea_SqFt'].fillna(0)\n",
    "        # change YEAR_y to YEAR\n",
    "        df['YEAR'] = df['YEAR_y']\n",
    "        # Sanitize column names\n",
    "        df.columns = [re.sub(r'[^a-zA-Z0-9_]', '_', col) for col in df.columns]\n",
    "        # set output feature class name\n",
    "        yearstr = str(year)\n",
    "        outfc = f\"Parcel_History_Attributed{version}{yearstr}\"    \n",
    "        # export updated parcel history to feature class filtered by year\n",
    "        df.spatial.to_featureclass(location=os.path.join(\"C:/GIS/Scratch.gdb\", outfc), overwrite=True, sanitize_columns=False)\n",
    "\n",
    "# identify parcels that did not join from the merge\n",
    "def get_unjoined(parcel_history, unitsTable, years):\n",
    "    for year in years:\n",
    "        # Filter parcel_history for the current year\n",
    "        parcel_history_filtered = parcel_history.loc[parcel_history['YEAR'] == year]\n",
    "        \n",
    "        # Merge with unitsTable for the same year\n",
    "        units_by_year = unitsTable.loc[unitsTable.YEAR == year]\n",
    "        merged_data = units_by_year.merge(parcel_history_filtered, on='APN', how='outer', indicator=True)\n",
    "        \n",
    "        # Print year and merge value counts\n",
    "        print(year)\n",
    "        print(merged_data._merge.value_counts())\n",
    "        \n",
    "        # Data manipulations\n",
    "        merged_data = merged_data.rename(columns={'YEAR_x': 'YEAR'})\n",
    "        merged_data = merged_data.loc[merged_data._merge != 'both']\n",
    "        merged_data.info()\n",
    "        merged_data = merged_data[['APN', 'YEAR', '_merge', 'CommercialFloorArea_SqFt_x', 'Residential_Units_x', 'TouristAccommodation_Units_x',\n",
    "                                   'CommercialFloorArea_SqFt_y', 'Residential_Units_y', 'TouristAccommodation_Units_y']]\n",
    "        merged_data.info()\n",
    "        # Save to CSV\n",
    "        merged_data.to_csv(f\"data\\\\Parcel_History_Attributed_APN_Merge{version, year}.csv\", index=False)\n",
    "\n",
    "# check for duplicates in parcel_history\n",
    "def check_duplicates(parcel_history, unitsTable, years):\n",
    "    for year in years:\n",
    "        print(year)\n",
    "        # make a list of duplicate APNs\n",
    "        duplicateAPNs = parcel_history.loc[parcel_history['YEAR'] == year].APN[parcel_history.loc[parcel_history['YEAR'] == year].APN.duplicated()].tolist()\n",
    "        # print out duplicate rows\n",
    "        print(duplicateAPNs)\n",
    "        # save 2021 duplicates to csv\n",
    "        if year == 2018:\n",
    "            parcel_history.loc[parcel_history['YEAR'] == year].loc[parcel_history.loc[parcel_history['YEAR'] == year].APN.duplicated()].to_csv(\"data\\Parcel_History_Duplicates_2018.csv\")\n",
    "\n",
    "        # make a list of duplicate APNs in unitsTable\n",
    "        duplicateAPNsCA = unitsTable.loc[unitsTable['YEAR'] == year].APN[unitsTable.loc[unitsTable['YEAR'] == year].APN.duplicated()].tolist()\n",
    "        # print out duplicate rows\n",
    "        print(duplicateAPNsCA)\n",
    "\n",
    "# compare total Residnetial Units, Commercial Floor Area, and Tourist Accommodation Units by year, bewtween parcel_history and unitsTable\n",
    "def compare_totals(parcel_history, unitsTable, years):\n",
    "    for year in years:\n",
    "        # filter parcel_history by year\n",
    "        parcel_history_year = parcel_history.loc[parcel_history['YEAR'] == year]\n",
    "        # filter unitsTable by year\n",
    "        unitsTable_year = unitsTable.loc[unitsTable['YEAR'] == year]\n",
    "        # # remove any commas from CommercialFloorArea_SqFt in unitsTable_year using .loc\n",
    "        # unitsTable_year.loc[:, 'CommercialFloorArea_SqFt'] = unitsTable_year['CommercialFloorArea_SqFt'].str.replace(',', '').astype(float)\n",
    "\n",
    "        # get sum of Residential Units in parcel_history\n",
    "        resTotal = parcel_history_year['Residential_Units'].sum()\n",
    "        cfaTotal = parcel_history_year['CommercialFloorArea_SqFt'].sum()\n",
    "        tauTotal = parcel_history_year['TouristAccommodation_Units'].sum()\n",
    "\n",
    "        # get sum of Residential Units in unitsTable\n",
    "        resTotalCA = unitsTable_year['Residential_Units'].sum()\n",
    "        cfaTotalCA = unitsTable_year['CommercialFloorArea_SqFt'].sum()\n",
    "        tauTotalCA = unitsTable_year['TouristAccommodation_Units'].sum()\n",
    "\n",
    "        # print totals\n",
    "        print(year)\n",
    "        print('Residential Units in Parcel_History \\n' + str(resTotal))\n",
    "        print('Residential Units in updated table \\n'+ str(resTotalCA))\n",
    "        print('Commercial Floor Area in Parcel_History \\n'+ str(cfaTotal))\n",
    "        print('Commercial Floor Area in updated table \\n'+ str(cfaTotalCA))\n",
    "        print('Tourist Accommodation Units in Parcel_History \\n'+ str(tauTotal))\n",
    "        print('Tourist Accommodation Units in updated table \\n'+ str(tauTotalCA))\n",
    "\n",
    "# identify rows where the Residential Units, Commercial Floor Area, and Tourist Accommodation Units are different between parcel_history and unitsTable\n",
    "def find_different_rows(parcel_history, unitsTable, years):\n",
    "    for year in years:\n",
    "        print(year)\n",
    "        # filter parcel_history by year\n",
    "        parcel_history_year = parcel_history.loc[parcel_history['YEAR'] == year]\n",
    "        # filter unitsTable by year\n",
    "        unitsTable_year = unitsTable.loc[unitsTable['YEAR'] == year]\n",
    "        # # remove any commas from CommercialFloorArea_SqFt in unitsTable_year using .loc\n",
    "        # unitsTable_year.loc[:, 'CommercialFloorArea_SqFt'] = unitsTable_year['CommercialFloorArea_SqFt'].str.replace(',', '').astype(float)\n",
    "        # merge parcel_history_year and unitsTable_year\n",
    "        df = pd.merge(parcel_history_year, unitsTable_year, right_on='APN', left_on='APN', how='outer', indicator=True)\n",
    "        # drop columns that are not needed\n",
    "        df = df[['APN', 'YEAR_x','YEAR_y', 'Residential_Units_x', 'CommercialFloorArea_SqFt_x', 'TouristAccommodation_Units_x', 'Residential_Units_y', 'CommercialFloorArea_SqFt_y', 'TouristAccommodation_Units_y']]\n",
    "        # get fields where the Residential Units, Commercial Floor Area, and Tourist Accommodation Units do not match\n",
    "        df = df.loc[(df['Residential_Units_x'] != df['Residential_Units_y']) | (df['CommercialFloorArea_SqFt_x'] != df['CommercialFloorArea_SqFt_y']) | (df['TouristAccommodation_Units_x'] != df['TouristAccommodation_Units_y'])]\n",
    "        # print out the rows\n",
    "        print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the merge functions to export feature classes and get unjoined data as csv\n",
    "# merge_and_export(parcel_history, unitsTable, years)\n",
    "get_unjoined(parcel_history, unitsTable, years)\n",
    "check_duplicates(parcel_history, unitsTable, years)\n",
    "compare_totals(parcel_history, unitsTable, years)\n",
    "find_different_rows(parcel_history, unitsTable, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze the changes in parcel history by year\n",
    "years = [2012, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "df = sdfUnits\n",
    "for year in years:\n",
    "    print(year)\n",
    "    # filter parcel_history by year\n",
    "    parcel_history_year = df.loc[df['YEAR'] == year]\n",
    "    # get sum of Residential Units in parcel_history\n",
    "    resTotal = parcel_history_year['Residential_Units'].sum()\n",
    "    cfaTotal = parcel_history_year['CommercialFloorArea_SqFt'].sum()\n",
    "    tauTotal = parcel_history_year['TouristAccommodation_Units'].sum()\n",
    "    # print totals\n",
    "    print('Residential Units in Parcel_History \\n' + str(resTotal))\n",
    "    print('Commercial Floor Area in Parcel_History \\n'+ str(cfaTotal))\n",
    "    print('Tourist Accommodation Units in Parcel_History \\n'+ str(tauTotal))\n",
    "    # print out changes in units by APN\n",
    "# firnd all the rows where duplicate APNs change units between years\n",
    "for year in years:\n",
    "    print(year)\n",
    "    # make a list of duplicate APNs as sets of APNs\n",
    "    duplicateAPNs = df.loc[df['YEAR'] == year].APN[df.loc[df['YEAR'] == year].APN.duplicated()].tolist()\n",
    "    # loop through the duplicate APNs\n",
    "    for apn in duplicateAPNs:\n",
    "        # get the rows for the APN\n",
    "        df = df.loc[df['APN'] == apn]\n",
    "        # get the rows for the APN by year\n",
    "        df = df.loc[df['YEAR'] == year]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total residential units by year\n",
    "def get_totals(parcels, years):\n",
    "    # total\n",
    "    total = pd.DataFrame(columns=['Year', 'Residential_Units'])\n",
    "    for year in years:\n",
    "        # filter parcel_history by year\n",
    "        parcel_history_year = parcels.loc[parcels['YEAR'] == year]\n",
    "        # get sum of Residential Units in parcel_history\n",
    "        resTotal = parcel_history_year['Residential_Units'].sum()\n",
    "\n",
    "        # add new row using concat\n",
    "        total = pd.concat([total, pd.DataFrame({'Year': [year], 'Residential_Units': [resTotal]})])\n",
    "    return total\n",
    "\n",
    "# get total residential units by year\n",
    "total = get_totals(parcel_history, years)\n",
    "# calculate percentage change in residential units year over year\n",
    "total['Percent_Change'] = (total['Residential_Units'].pct_change())*100\n",
    "# create a new column for the difference in residential units year over year\n",
    "total['Difference'] = total['Residential_Units'].diff()\n",
    "\n",
    "total\n",
    "# export to csv\n",
    "total.to_csv('total_residential_units_by_year.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Transformation***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Proecssing***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deed Restrictions\n",
    "> Deed restricted unit research needs to be merged with LTinfo housing deed restricitons and parcel unit data from 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedUnits  = read_excel(\"data\\Housing_Deed_Restrcitions.xlsx\", 0)\n",
    "dfDeedLTinfo = pd.read_json(\"https://laketahoeinfo.org/WebServices/GetDeedRestrictedParcels/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedUnits.to_csv(\"data\\DeedRestricted_HousingUnits.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedUnits.Units.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedLTinfo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique values for deed restrcition type\n",
    "dfDeedLTinfo.DeedRestrictionType.unique()\n",
    "\n",
    "# filter to Affordable, Achievable, and Moderate\n",
    "dfDeedLTinfo = dfDeedLTinfo[dfDeedLTinfo.DeedRestrictionType.isin(['Affordable Housing', 'Moderate Income Housing', 'Achievable Housing'])]  \n",
    "\n",
    "# count of total records\n",
    "dfDeedLTinfo.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcelUnits22.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedUnitsMerge = dfDeedUnits.merge(dfDeedLTinfo, on='APN', how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedUnitsMerge._merge.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedLTinfo[dfDeedLTinfo.duplicated(subset=['APN','DeedRestrictionType'], keep=False)].sort_values('APN').to_csv(\"HousingDeedRestrictions_LTinfo_Duplicates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify duplicates unique by APN and \n",
    "dfDeedUnits[dfDeedUnits.duplicated(subset=['APN', 'Deed_Restriction_Type','Units'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify duplicates\n",
    "dfDeedUnitsMerge[dfDeedUnitsMerge.duplicated(subset=['APN'], keep=False)].sort_values(by='APN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedUnitsMerge.to_csv(\"HousingDeedRestrictions_All.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the deed restricted units with the parcel units\n",
    "dfDeedUnits_ParcelUnits  = dfDeedUnits.merge(parcelUnits22, on='APN', how='left')\n",
    "# merge the deed restricted units with the parcel units\n",
    "dfDeedLTinfo_ParcelUnits = dfDeedLTinfo.merge(parcelUnits22, left_on='APN', right_on='APN', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedLTinfo_ParcelUnits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDeedLTinfo_ParcelUnits.Residential_Units.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADU Tracking\n",
    "> ADU permit tracking from TRPA and othe Jurisdictions. There is a need to establish a system of record for this information (LT Info). This is similar to the Residential Bonus Unit data and there’s crossover on some of these, where a bonus unit was used to create an ADU, but you can have an ADU without requiring a bonus unit, and you can use a bonus unit without it being an ADU… "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfADU = read_excel(\"data\\ADU Tracking.xlsx\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfADU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Allocations\n",
    "> This file includes all of the allocations that have been tracked in LT Info, and adds in whether the subject parcel has been issued a BMP/SCC certificate and/or whether Air Quality/Mobility Mitigation fees (for added VMT) or Water Quality Mitigation fees (for added coverage) have been paid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allocations = read_excel(\"data\\Allocation_Tracking.xlsx\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transactions with Inactive APNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inactiveParcels = read_file(\"data\\Transactions_InactiveParcels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Process to compare against assessor parcel data signifying development\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parcels feature class of missing parcels for Residential Units\n",
    "\n",
    "# get parcel master\n",
    "parcelURL = \"https://maps.trpa.org/server/rest/services/Parcels/MapServer/0\"\n",
    "vhrURL    = \"https://maps.trpa.org/server/rest/services/VHR/MapServer/0\"\n",
    "# get parcel and VHR data as spatial dataframes\n",
    "sdfParcel = get_fs_data_spatial(parcelURL)\n",
    "# sdfVHR    = get_fs_data_spatial(vhrURL)\n",
    "\n",
    "# # keep only the columns needed\n",
    "# sdfParcel = sdfParcel[['APN','EXISTING_LANDUSE','YEAR_BUILT','BEDROOMS','UNITS','SHAPE']]\n",
    "# sdfVHR    = sdfVHR[['APN','SHAPE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets feature service data as spatially enabled dataframe\n",
    "def get_fs_data_spatial(service_url):\n",
    "    feature_layer = FeatureLayer(service_url)\n",
    "    df = feature_layer.query().sdf\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the parcel and VHR data\n",
    "sdf = pd.merge(sdfParcel, sdfVHR, on='APN', how='left', indicator=True)\n",
    "# merge the 2023 parcelhistory and \n",
    "parcelDev2023 = parcel_history.loc[parcel_history['YEAR'] == 2023]\n",
    "sdf = pd.merge(sdf, parcelDev2023, on='APN', how='left', indicator=True)\n",
    "sdf.info()\n",
    "# # keep fields needed for QA\n",
    "# sdf = sdf[['APN','EXISTING_LANDUSE','YEAR_BUILT','BEDROOMS','UNITS','WITHIN_TRPABNDY','_merge','SHAPE']]\n",
    "# # export to feature class\n",
    "# sdf.spatial.to_featureclass(location=os.path.join(arcpy.env.workspace, 'Parcel_Review'), overwrite=True, sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
